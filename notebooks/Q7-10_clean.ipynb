{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "seed = 10676128  # Set random seed for reproducibility\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Utility Functions\n",
    "# -----------------\n",
    "\n",
    "def compute_rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "def compute_r2(y_true, y_pred):\n",
    "    ss_total = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    ss_residual = np.sum((y_true - y_pred) ** 2)\n",
    "    return 1 - (ss_residual / ss_total)\n",
    "\n",
    "def normal_regression(X_train, y_train):\n",
    "    # (X^T X)^(-1) X^T y\n",
    "    return np.linalg.inv(X_train.T @ X_train) @ X_train.T @ y_train\n",
    "\n",
    "def ridge_regression(X_train, y_train, alpha):\n",
    "    # (X^T X + alpha * I)^(-1) X^T y\n",
    "    n_features = X_train.shape[1]\n",
    "    identity = np.eye(n_features)\n",
    "    # We often do not regularize the bias term => set identity[0, 0] = 0\n",
    "    identity[0, 0] = 0\n",
    "    return np.linalg.inv(X_train.T @ X_train + alpha * identity) @ X_train.T @ y_train\n",
    "\n",
    "def lasso_regression(X_train, y_train, alpha, max_iter=10000, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Coordinate Descent for Lasso.\n",
    "    Note: For real-world usage, consider sklearn.linear_model.Lasso.\n",
    "    \"\"\"\n",
    "    m, n = X_train.shape\n",
    "    beta = np.zeros(n)\n",
    "    for _ in range(max_iter):\n",
    "        beta_old = beta.copy()\n",
    "        for j in range(n):\n",
    "            residual = y_train - X_train @ beta + X_train[:, j] * beta[j]\n",
    "            rho = X_train[:, j].T @ residual\n",
    "            if j == 0:  # Intercept (no regularization)\n",
    "                beta[j] = rho / (X_train[:, j].T @ X_train[:, j])\n",
    "            else:\n",
    "                # Soft-thresholding\n",
    "                beta[j] = np.sign(rho) * max(0, abs(rho) - alpha) / (X_train[:, j].T @ X_train[:, j])\n",
    "        if np.max(np.abs(beta - beta_old)) < tol:\n",
    "            break\n",
    "    return beta\n",
    "\n",
    "def plot_feature_vs_dependent(df, dependent_var):\n",
    "    \"\"\"\n",
    "    Generates scatter plots for each feature in the DataFrame against the dependent variable.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        dependent_var (str): The dependent variable column name.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Exclude the dependent variable from the features\n",
    "    features = df.columns.drop([dependent_var])\n",
    "\n",
    "    # Define number of rows and columns for subplots\n",
    "    n_features = len(features)\n",
    "    n_cols = 3  # Number of columns in the subplot grid\n",
    "    n_rows = -(-n_features // n_cols) # Ceiling division\n",
    "    \n",
    "    # Set figure size based on rows and columns\n",
    "    plt.figure(figsize=(20, 5 * n_rows))\n",
    "\n",
    "    # Create a scatterplot for each feature\n",
    "    for idx, feature in enumerate(features, start=1):\n",
    "        plt.subplot(n_rows, n_cols, idx)\n",
    "        sns.scatterplot(x=df[feature], y=df[dependent_var])\n",
    "        plt.title(f'{dependent_var} vs {feature}')\n",
    "        plt.xlabel(feature)\n",
    "        plt.ylabel(dependent_var)\n",
    "        plt.grid(True)\n",
    "\n",
    "    # Adjust layout to avoid overlap\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Forward Feature Selection\n",
    "\n",
    "def plot_forward_selection_results(results_df):\n",
    "    \"\"\"\n",
    "    Plots RMSE and R² over the number of features selected.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # RMSE plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, len(results_df) + 1), results_df['RMSE'], marker='o', label='RMSE')\n",
    "    plt.title('Forward Feature Selection: RMSE')\n",
    "    plt.xlabel('Number of Features Selected')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # R² plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, len(results_df) + 1), results_df['R2'], marker='o', label='R²')\n",
    "    plt.title('Forward Feature Selection: R²')\n",
    "    plt.xlabel('Number of Features Selected')\n",
    "    plt.ylabel('R²')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def forward_feature_selection_kfold(X, y, k=5, max_features=None):\n",
    "    \"\"\"\n",
    "    Perform forward feature selection using k-fold cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - X: DataFrame of features\n",
    "    - y: Series of target variable\n",
    "    - k: Number of folds for cross-validation\n",
    "    - max_features: Max number of features to select. If None, selects all.\n",
    "\n",
    "    Returns:\n",
    "    - selected_features: List of features selected in order\n",
    "    - results: List of results for each increment of feature selection\n",
    "    \"\"\"\n",
    "    remaining_features = list(X.columns)\n",
    "    selected_features = []\n",
    "    results = []\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    while remaining_features:\n",
    "        best_rmse = float('inf')\n",
    "        best_feature = None\n",
    "        best_betas = None\n",
    "        best_alpha = None\n",
    "        \n",
    "        for feature in remaining_features:\n",
    "            # Subset with current selected + new feature\n",
    "            X_temp = X[selected_features + [feature]]\n",
    "            rmse_list = []\n",
    "            r2_list = []\n",
    "            \n",
    "            for train_idx, val_idx in kf.split(X_temp):\n",
    "                X_train_fold = X_temp.iloc[train_idx]\n",
    "                X_val_fold   = X_temp.iloc[val_idx]\n",
    "                y_train_fold = y.iloc[train_idx]\n",
    "                y_val_fold   = y.iloc[val_idx]\n",
    "                \n",
    "                model = LinearRegression()\n",
    "                model.fit(X_train_fold, y_train_fold)\n",
    "                \n",
    "                y_pred_fold = model.predict(X_val_fold)\n",
    "                rmse_fold = np.sqrt(mean_squared_error(y_val_fold, y_pred_fold))\n",
    "                r2_fold   = r2_score(y_val_fold, y_pred_fold)\n",
    "                \n",
    "                rmse_list.append(rmse_fold)\n",
    "                r2_list.append(r2_fold)\n",
    "            \n",
    "            avg_rmse = np.mean(rmse_list)\n",
    "            avg_r2   = np.mean(r2_list)\n",
    "            \n",
    "            if avg_rmse < best_rmse:\n",
    "                best_rmse   = avg_rmse\n",
    "                best_feature = feature\n",
    "                best_betas   = model.coef_\n",
    "                best_alpha   = model.intercept_\n",
    "        \n",
    "        selected_features.append(best_feature)\n",
    "        remaining_features.remove(best_feature)\n",
    "        \n",
    "        results.append([\n",
    "            selected_features.copy(),\n",
    "            best_betas,\n",
    "            best_alpha,\n",
    "            best_rmse,\n",
    "            avg_r2\n",
    "        ])\n",
    "        \n",
    "        if max_features is not None and len(selected_features) >= max_features:\n",
    "            break\n",
    "\n",
    "    return selected_features, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Main Classes - RegressionAnalysis(Regression) and PepperAnalysis(Classification)\n",
    "# -----------------\n",
    "\n",
    "class RegressionAnalysis:\n",
    "    def __init__(self, X, y, alphas=None, seed=seed, n_splits=5):\n",
    "        \"\"\"\n",
    "        X, y: features and target arrays\n",
    "        alphas: array-like of alpha values for Ridge/Lasso\n",
    "        seed: random seed for reproducibility\n",
    "        n_splits: number of splits for cross-validation\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.seed = seed\n",
    "        self.n_splits = n_splits\n",
    "        self.results_cv = []\n",
    "        self.results_test = []\n",
    "\n",
    "        if alphas is None:\n",
    "            self.alphas = np.array([0.00001, 0.0001, 0.001, 0.01, \n",
    "                                    0.1, 1, 2, 5, 10, 20, 100, 1000])\n",
    "        else:\n",
    "            self.alphas = alphas\n",
    "\n",
    "    def cross_validate(self):\n",
    "        \"\"\"\n",
    "        Perform KFold cross-validation on the entire dataset (X, y)\n",
    "        for Normal, Ridge, Lasso across each alpha (for Ridge/Lasso).\n",
    "        We store all fold results in self.results_cv as tuples:\n",
    "          (model_type, alpha, fold_idx, rmse, r2, betas).\n",
    "        \"\"\"\n",
    "        kf = KFold(n_splits=self.n_splits, shuffle=True, random_state=self.seed)\n",
    "        self.results_cv = []\n",
    "        \n",
    "        fold_num = 1\n",
    "        for train_index, val_index in kf.split(self.X):\n",
    "            X_train, X_val = self.X[train_index], self.X[val_index]\n",
    "            y_train, y_val = self.y[train_index], self.y[val_index]\n",
    "\n",
    "            # Scale\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "            # Add intercept\n",
    "            X_train_scaled = np.hstack((np.ones((X_train_scaled.shape[0], 1)), X_train_scaled))\n",
    "            X_val_scaled = np.hstack((np.ones((X_val_scaled.shape[0], 1)), X_val_scaled))\n",
    "\n",
    "            # 1) Normal Regression\n",
    "            beta_normal = normal_regression(X_train_scaled, y_train)\n",
    "            y_pred_normal = X_val_scaled @ beta_normal\n",
    "            rmse_normal = compute_rmse(y_val, y_pred_normal)\n",
    "            r2_normal = compute_r2(y_val, y_pred_normal)\n",
    "            self.results_cv.append(('Normal', None, fold_num, rmse_normal, r2_normal, beta_normal))\n",
    "\n",
    "            # 2) Ridge for each alpha\n",
    "            for alpha in self.alphas:\n",
    "                beta_ridge = ridge_regression(X_train_scaled, y_train, alpha)\n",
    "                y_pred_ridge = X_val_scaled @ beta_ridge\n",
    "                rmse_ridge = compute_rmse(y_val, y_pred_ridge)\n",
    "                r2_ridge = compute_r2(y_val, y_pred_ridge)\n",
    "                self.results_cv.append(('Ridge', alpha, fold_num, rmse_ridge, r2_ridge, beta_ridge))\n",
    "\n",
    "            # 3) Lasso for each alpha\n",
    "            for alpha in self.alphas:\n",
    "                beta_lasso = lasso_regression(X_train_scaled, y_train, alpha)\n",
    "                y_pred_lasso = X_val_scaled @ beta_lasso\n",
    "                rmse_lasso = compute_rmse(y_val, y_pred_lasso)\n",
    "                r2_lasso = compute_r2(y_val, y_pred_lasso)\n",
    "                self.results_cv.append(('Lasso', alpha, fold_num, rmse_lasso, r2_lasso, beta_lasso))\n",
    "            \n",
    "            fold_num += 1\n",
    "\n",
    "    def get_cv_results_df(self):\n",
    "        \"\"\"Return cross-validation results as a pandas DataFrame.\"\"\"\n",
    "        columns = ['Model','Alpha','Fold','RMSE','R2','Betas']\n",
    "        return pd.DataFrame(self.results_cv, columns=columns)\n",
    "\n",
    "    def pick_best_alpha(self, model_type='Ridge', metric='RMSE'):\n",
    "        \"\"\"\n",
    "        From cross-validation results, pick the alpha that has the best\n",
    "        (lowest) mean RMSE or (highest) mean R2 across folds for a given model_type.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model_type : {'Ridge', 'Lasso'}\n",
    "            The type of model for which to pick best alpha.\n",
    "        metric : {'RMSE', 'R2'}\n",
    "            The metric to optimize. 'RMSE' picks the alpha with min RMSE;\n",
    "            'R2' picks the alpha with max R2.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        best_alpha : float\n",
    "            The alpha that performed best in cross-validation.\n",
    "        best_score : float\n",
    "            The corresponding CV score (RMSE or R2) for that alpha.\n",
    "        \"\"\"\n",
    "        # Pull the cross-validation results into a DataFrame\n",
    "        df = self.get_cv_results_df()\n",
    "\n",
    "        # Group by (Model, Alpha), then compute the mean of RMSE and R2 across folds\n",
    "        df_agg = df.groupby(['Model','Alpha'])[['RMSE','R2']].mean().reset_index()\n",
    "\n",
    "        # Filter only to the chosen model_type\n",
    "        # (Make sure user isn't trying to pick alpha for \"Normal\", which has no alpha)\n",
    "        if model_type not in ['Ridge', 'Lasso']:\n",
    "            raise ValueError(\"model_type must be 'Ridge' or 'Lasso' for picking best alpha.\")\n",
    "\n",
    "        df_agg = df_agg[df_agg['Model'] == model_type]\n",
    "\n",
    "        if df_agg.empty:\n",
    "            raise ValueError(f\"No cross-validation results found for model '{model_type}'. \"\n",
    "                            f\"Check that you ran CV and that your model_type is correct.\")\n",
    "\n",
    "        if metric not in ['RMSE','R2']:\n",
    "            raise ValueError(\"metric must be 'RMSE' or 'R2'.\")\n",
    "\n",
    "        # Find the alpha that minimizes or maximizes the chosen metric\n",
    "        if metric == 'RMSE':\n",
    "            # We want the alpha with the *lowest* mean RMSE\n",
    "            best_idx = df_agg['RMSE'].idxmin()\n",
    "            best_alpha = df_agg.loc[best_idx, 'Alpha']\n",
    "            best_score = df_agg.loc[best_idx, 'RMSE']\n",
    "        else:  # metric == 'R2'\n",
    "            # We want the alpha with the *highest* mean R2\n",
    "            best_idx = df_agg['R2'].idxmax()\n",
    "            best_alpha = df_agg.loc[best_idx, 'Alpha']\n",
    "            best_score = df_agg.loc[best_idx, 'R2']\n",
    "\n",
    "        return best_alpha, best_score\n",
    "\n",
    "\n",
    "    def finalize_and_evaluate(self, X_train, y_train, X_test, y_test, \n",
    "                              best_ridge_alpha, best_lasso_alpha, \n",
    "                              make_residual_plots=True):\n",
    "        \"\"\"\n",
    "        Given the best alpha for Ridge and Lasso (from cross_val),\n",
    "        train Normal, best Ridge, best Lasso on the entire training set,\n",
    "        evaluate on the test set, and optionally produce residual plots\n",
    "        in subplots.\n",
    "        \"\"\"\n",
    "        # Scale the data\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Add intercept\n",
    "        X_train_scaled = np.hstack((np.ones((X_train_scaled.shape[0], 1)), X_train_scaled))\n",
    "        X_test_scaled = np.hstack((np.ones((X_test_scaled.shape[0], 1)), X_test_scaled))\n",
    "\n",
    "        self.results_test = []\n",
    "\n",
    "        # 1) Normal\n",
    "        beta_normal = normal_regression(X_train_scaled, y_train)\n",
    "        y_pred_normal = X_test_scaled @ beta_normal\n",
    "        rmse_normal = compute_rmse(y_test, y_pred_normal)\n",
    "        r2_normal = compute_r2(y_test, y_pred_normal)\n",
    "        self.results_test.append(('Normal', None, rmse_normal, r2_normal, beta_normal))\n",
    "\n",
    "        # 2) Ridge (best_ridge_alpha)\n",
    "        beta_ridge = ridge_regression(X_train_scaled, y_train, best_ridge_alpha)\n",
    "        y_pred_ridge = X_test_scaled @ beta_ridge\n",
    "        rmse_ridge = compute_rmse(y_test, y_pred_ridge)\n",
    "        r2_ridge = compute_r2(y_test, y_pred_ridge)\n",
    "        self.results_test.append(('Ridge', best_ridge_alpha, rmse_ridge, r2_ridge, beta_ridge))\n",
    "\n",
    "        # 3) Lasso (best_lasso_alpha)\n",
    "        beta_lasso = lasso_regression(X_train_scaled, y_train, best_lasso_alpha)\n",
    "        y_pred_lasso = X_test_scaled @ beta_lasso\n",
    "        rmse_lasso = compute_rmse(y_test, y_pred_lasso)\n",
    "        r2_lasso = compute_r2(y_test, y_pred_lasso)\n",
    "        self.results_test.append(('Lasso', best_lasso_alpha, rmse_lasso, r2_lasso, beta_lasso))\n",
    "\n",
    "        # Optional: combined residual plots in subplots\n",
    "        if make_residual_plots:\n",
    "            # We'll do Normal, Ridge, Lasso side by side\n",
    "            fig, axs = plt.subplots(2, 3, figsize=(18, 10))  # 2 rows, 3 columns\n",
    "\n",
    "            # Normal\n",
    "            residuals_normal = y_test - y_pred_normal\n",
    "            std_res_normal = (residuals_normal - residuals_normal.mean()) / residuals_normal.std()\n",
    "            axs[0, 0].scatter(y_pred_normal, std_res_normal, color='blue')\n",
    "            axs[0, 0].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "            axs[0, 0].set_title(\"Normal Residual Plot\")\n",
    "            axs[0, 0].set_xlabel(\"Predicted\")\n",
    "            axs[0, 0].set_ylabel(\"Std Residuals\")\n",
    "            axs[1, 0].hist(residuals_normal, bins=15, color='green', edgecolor='black', density=True)\n",
    "            axs[1, 0].set_title(\"Normal Residual Histogram\")\n",
    "\n",
    "            # Ridge\n",
    "            residuals_ridge = y_test - y_pred_ridge\n",
    "            std_res_ridge = (residuals_ridge - residuals_ridge.mean()) / residuals_ridge.std()\n",
    "            axs[0, 1].scatter(y_pred_ridge, std_res_ridge, color='blue')\n",
    "            axs[0, 1].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "            axs[0, 1].set_title(f\"Ridge (alpha={best_ridge_alpha}) Residual\")\n",
    "            axs[0, 1].set_xlabel(\"Predicted\")\n",
    "            axs[0, 1].set_ylabel(\"Std Residuals\")\n",
    "            axs[1, 1].hist(residuals_ridge, bins=15, color='green', edgecolor='black', density=True)\n",
    "            axs[1, 1].set_title(\"Ridge Residual Histogram\")\n",
    "\n",
    "            # Lasso\n",
    "            residuals_lasso = y_test - y_pred_lasso\n",
    "            std_res_lasso = (residuals_lasso - residuals_lasso.mean()) / residuals_lasso.std()\n",
    "            axs[0, 2].scatter(y_pred_lasso, std_res_lasso, color='blue')\n",
    "            axs[0, 2].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "            axs[0, 2].set_title(f\"Lasso (alpha={best_lasso_alpha}) Residual\")\n",
    "            axs[0, 2].set_xlabel(\"Predicted\")\n",
    "            axs[0, 2].set_ylabel(\"Std Residuals\")\n",
    "            axs[1, 2].hist(residuals_lasso, bins=15, color='green', edgecolor='black', density=True)\n",
    "            axs[1, 2].set_title(\"Lasso Residual Histogram\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    def get_test_results_df(self):\n",
    "        \"\"\"Return final test results as a DataFrame.\"\"\"\n",
    "        columns = ['Model','Alpha','RMSE','R2','Betas']\n",
    "        return pd.DataFrame(self.results_test, columns=columns)\n",
    "\n",
    "    def plot_cv_rmse_r2(self):\n",
    "        \"\"\"\n",
    "        Plot average CV RMSE and R^2 vs. alpha on log scale.\n",
    "        \"\"\"\n",
    "        df = self.get_cv_results_df()\n",
    "        df_agg = df.groupby(['Model','Alpha'])[['RMSE','R2']].mean().reset_index()\n",
    "\n",
    "        normal_subset = df_agg[df_agg['Model'] == 'Normal']\n",
    "        ridge_subset = df_agg[df_agg['Model'] == 'Ridge']\n",
    "        lasso_subset = df_agg[df_agg['Model'] == 'Lasso']\n",
    "\n",
    "        # RMSE plot\n",
    "        plt.figure(figsize=(10,6))\n",
    "        plt.xscale('log')\n",
    "\n",
    "        plt.plot(ridge_subset['Alpha'], ridge_subset['RMSE'], marker='o', label='Ridge RMSE')\n",
    "        plt.plot(lasso_subset['Alpha'], lasso_subset['RMSE'], marker='s', label='Lasso RMSE')\n",
    "        if len(normal_subset) > 0:\n",
    "            normal_rmse = normal_subset['RMSE'].mean()\n",
    "            plt.axhline(y=normal_rmse, color='r', linestyle='--', label=f'Normal RMSE={normal_rmse:.3f}')\n",
    "\n",
    "        plt.title('CV RMSE vs. Alpha')\n",
    "        plt.xlabel('Alpha')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        # R2 plot\n",
    "        plt.figure(figsize=(10,6))\n",
    "        plt.xscale('log')\n",
    "\n",
    "        plt.plot(ridge_subset['Alpha'], ridge_subset['R2'], marker='o', label='Ridge R2')\n",
    "        plt.plot(lasso_subset['Alpha'], lasso_subset['R2'], marker='s', label='Lasso R2')\n",
    "        if len(normal_subset) > 0:\n",
    "            normal_r2 = normal_subset['R2'].mean()\n",
    "            plt.axhline(y=normal_r2, color='r', linestyle='--', label=f'Normal R2={normal_r2:.3f}')\n",
    "\n",
    "        plt.title('CV R2 vs. Alpha')\n",
    "        plt.xlabel('Alpha')\n",
    "        plt.ylabel('R2')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_coefs(self, betas, feature_names=None, model_name='Normal', alpha=None):\n",
    "\n",
    "        if feature_names is None:\n",
    "            feature_names = [f\"x{i}\" for i in range(len(betas))]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(feature_names, betas)\n",
    "        \n",
    "        if model_name == 'Normal':\n",
    "            plt.title(\"Coefficients: Normal Regression\")\n",
    "        else:\n",
    "            if alpha is not None:\n",
    "                plt.title(f\"Coefficients: {model_name} (alpha={alpha})\")\n",
    "            else:\n",
    "                plt.title(f\"Coefficients: {model_name}\")\n",
    "\n",
    "        plt.xlabel(\"Features\")\n",
    "        plt.ylabel(\"Coefficient Value\")\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "class PepperAnalysis:\n",
    "    def __init__(\n",
    "        self, \n",
    "        df_capstone, \n",
    "        tagsdf, \n",
    "        seed=seed\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize PepperAnalysis with the two dataframes and a random seed.\n",
    "        df_capstone: main DataFrame with course/professor data\n",
    "        tagsdf: additional DataFrame to be joined\n",
    "        \"\"\"\n",
    "        self.seed = seed\n",
    "        self.df_capstone = df_capstone\n",
    "        self.tagsdf = tagsdf\n",
    "        self.tagsdf.columns=self.tagsdf.columns.astype(str)\n",
    "        self.df = None  # Will hold the merged/cleaned DataFrame later\n",
    "        \n",
    "        # Placeholders for trained models (if you want to access them outside)\n",
    "        self.log_reg_single = None\n",
    "        self.log_reg_multi = None\n",
    "        self.svm_model = None\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Merge (inner join) df_capstone and tagsdf, drop NaN, compute proportions,\n",
    "        filter for Male or Female professor, etc. \n",
    "        Stores the cleaned data in self.df.\n",
    "        \"\"\"\n",
    "        # 1) Merge\n",
    "        Q10df = self.df_capstone.join(self.tagsdf, how='inner')\n",
    "\n",
    "        # 2) Drop missing\n",
    "        Q10df.dropna(inplace=True)\n",
    "\n",
    "        # 3) Convert columns i >= 8 to proportion by dividing by \"Number of ratings\"\n",
    "        for i in Q10df.columns[8:]:\n",
    "            Q10df[i] = Q10df[i].div(Q10df['Number of ratings'])\n",
    "\n",
    "        # 4) Filter rows to (Male=1,Female=0) or (Male=0,Female=1)\n",
    "        #    means \"exactly one of them is 1\"\n",
    "        #    The parentheses for & | are crucial.\n",
    "        Q10df = Q10df[((Q10df['Male Professor'] == 1) & (Q10df['Female Professor'] == 0)) |\n",
    "                      ((Q10df['Male Professor'] == 0) & (Q10df['Female Professor'] == 1))]\n",
    "\n",
    "        self.df = Q10df.copy()  # store cleaned DataFrame in self.df\n",
    "\n",
    "    def plot_correlation_matrix(self):\n",
    "        \"\"\"\n",
    "        Plots a large correlation matrix for self.df, if you want to see all features.\n",
    "        \"\"\"\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"Data not preprocessed yet. Call preprocess_data() first.\")\n",
    "        \n",
    "        correlation_matrix = self.df.corr()\n",
    "        plt.figure(figsize=(40, 40))\n",
    "        sns.heatmap(correlation_matrix, cmap=\"RdBu_r\", annot=True)\n",
    "        plt.title(\"Correlation Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_scatter_single(self, x_col='Average Rating', y_col='Received a pepper'):\n",
    "        \"\"\"\n",
    "        Simple scatter plot of x_col vs. y_col in self.df.\n",
    "        By default: x=Average Rating, y=Received a pepper\n",
    "        \"\"\"\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"Data not preprocessed yet. Call preprocess_data() first.\")\n",
    "\n",
    "        x_vals = self.df[x_col]\n",
    "        y_vals = self.df[y_col]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.scatter(x=x_vals, y=y_vals, c='purple')\n",
    "        ax.set_title(f\"Scatterplot of {x_col} vs. {y_col}\")\n",
    "        ax.set_xlabel(x_col)\n",
    "        ax.set_ylabel(y_col)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def logistic_regression_single_var(self, x_col='Average Rating', y_col='Received a pepper', threshold=0.5):\n",
    "        \"\"\"\n",
    "        Fits a single-variable Logistic Regression using x_col as predictor for y_col.\n",
    "        Plots confusion matrix, classification report, ROC, etc.\n",
    "        \n",
    "        threshold: The cutoff for assigning class = 1 if P(Y=1) > threshold.\n",
    "        \"\"\"\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"Data not preprocessed yet. Call preprocess_data() first.\")\n",
    "\n",
    "        X = self.df[[x_col]]  # must be 2D => double bracket\n",
    "        y = self.df[y_col]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=self.seed\n",
    "        )\n",
    "\n",
    "        # Fit logistic regression\n",
    "        log_reg_single = LogisticRegression()\n",
    "        log_reg_single.fit(X_train, y_train)\n",
    "        self.log_reg_single = log_reg_single  # store model\n",
    "\n",
    "        # Predictions\n",
    "        y_pred = log_reg_single.predict(X_test)\n",
    "        y_prob = log_reg_single.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Thresholding\n",
    "        y_pred_new = (y_prob > threshold).astype(int)\n",
    "\n",
    "        # Classification report\n",
    "        class_report = classification_report(y_test, y_pred_new)\n",
    "        print(\"Classification Report (Single-Var Logistic):\")\n",
    "        print(class_report)\n",
    "\n",
    "        # Coefficients\n",
    "        beta1 = log_reg_single.coef_[0][0]\n",
    "        intercept = log_reg_single.intercept_[0]\n",
    "        print(f\"Coefficient (beta1): {beta1:.4f}\")\n",
    "        print(f\"Intercept: {intercept:.4f}\")\n",
    "        print(f\"Odds multiplier (exp(beta1)): {np.exp(beta1):.4f}\")\n",
    "        print(f\"Intercept as odds => exp(intercept): {np.exp(intercept):.4f}\")\n",
    "\n",
    "        # Confusion Matrix\n",
    "        conf_matrix_single = confusion_matrix(y_test, y_pred_new)\n",
    "        sns.heatmap(\n",
    "            conf_matrix_single, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"0 (No Pepper)\", \"1 (Pepper)\"],\n",
    "            yticklabels=[\"0 (No Pepper)\", \"1 (Pepper)\"]\n",
    "        )\n",
    "        plt.title(\"Confusion Matrix (Single-Var Logistic)\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.show()\n",
    "\n",
    "        # Plot the Sigmoid curve\n",
    "        x_array = np.linspace(X_train.min()[0], X_train.max()[0], 100)\n",
    "        # logistic function\n",
    "        sig = 1 / (1 + np.exp(-(beta1 * x_array + intercept)))\n",
    "        plt.plot(x_array, sig, label=\"Sigmoid Curve\")\n",
    "\n",
    "        # Mark the threshold point\n",
    "        # Solve for x when sigmoid = threshold => x = [ln(threshold/(1-threshold)) - intercept]/beta1\n",
    "        threshold_x = (np.log(threshold / (1 - threshold)) - intercept) / beta1\n",
    "        plt.axvline(threshold_x, color='red', linestyle='--',\n",
    "                    label=f'Threshold at x={threshold_x:.2f}')\n",
    "\n",
    "        plt.title(\"Single-Var Logistic Sigmoid Curve\")\n",
    "        plt.xlabel(x_col)\n",
    "        plt.ylabel(\"Probability of Pepper (y=1)\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # ROC Curve\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "        plt.title(\"ROC Curve (Single-Var Logistic)\")\n",
    "        plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "        plt.ylabel(\"True Positive Rate (Sensitivity)\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # If you want the \"optimal threshold\" from Youden's J statistic (tpr - fpr)\n",
    "        optimal_threshold_index = np.argmax(tpr - fpr)\n",
    "        optimal_threshold = thresholds[optimal_threshold_index]\n",
    "        print(f\"Optimal Threshold (Youden's J): {optimal_threshold:.3f}\")\n",
    "\n",
    "    def logistic_regression_multi_var(self, drop_cols=None, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Fits a multi-variable logistic regression on self.df.\n",
    "        You can specify columns to drop (like 'Received a pepper', 'Number of ratings', etc.).\n",
    "        We apply MinMax scaling and do a train/test split, then show classification metrics, confusion matrix, ROC, etc.\n",
    "        \n",
    "        threshold: Probability threshold for predicting class=1\n",
    "        \"\"\"\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"Data not preprocessed yet. Call preprocess_data() first.\")\n",
    "\n",
    "        if drop_cols is None:\n",
    "            # By default, let's drop these columns to avoid target leakage or data not needed\n",
    "            drop_cols = ['Received a pepper', 'Number of ratings', \n",
    "                         'Number of ratings coming from online classes',\n",
    "                         'Male Professor', 'Female Professor']\n",
    "\n",
    "        # X => all columns except what's in drop_cols\n",
    "        # but also be mindful of columns that might be numeric only\n",
    "        df_clean = self.df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "        # The target\n",
    "        y = self.df['Received a pepper']\n",
    "\n",
    "        # Convert X to numeric\n",
    "        X = df_clean.select_dtypes(include=[np.number])\n",
    "\n",
    "        # Scale\n",
    "        scaler = MinMaxScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "        # Train/test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_scaled, y, test_size=0.2, random_state=self.seed\n",
    "        )\n",
    "\n",
    "        # Fit logistic regression\n",
    "        log_reg = LogisticRegression()\n",
    "        log_reg.fit(X_train, y_train)\n",
    "        self.log_reg_multi = log_reg  # store model\n",
    "\n",
    "        # Predictions\n",
    "        y_pred = log_reg.predict(X_test)\n",
    "        y_prob = log_reg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Thresholding\n",
    "        y_pred_new = (y_prob > threshold).astype(int)\n",
    "\n",
    "        # Classification report\n",
    "        class_report = classification_report(y_test, y_pred_new)\n",
    "        print(\"\\nClassification Report (Multi-Var Logistic):\")\n",
    "        print(class_report)\n",
    "\n",
    "        print(\"\\nCoefficients (log scale):\")\n",
    "        print(log_reg.coef_)\n",
    "        print(\"Exp of Coefficients (odds multipliers):\")\n",
    "        print(np.exp(log_reg.coef_))\n",
    "        print(\"Intercept:\", log_reg.intercept_)\n",
    "        print(\"Intercept (exp):\", np.exp(log_reg.intercept_))\n",
    "\n",
    "        # Confusion matrix\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred_new)\n",
    "        sns.heatmap(\n",
    "            conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"0 (No Pepper)\", \"1 (Pepper)\"],\n",
    "            yticklabels=[\"0 (No Pepper)\", \"1 (Pepper)\"]\n",
    "        )\n",
    "        plt.title(\"Confusion Matrix (Multi-Var Logistic)\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.show()\n",
    "\n",
    "        # ROC Curve\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "        plt.title(\"ROC Curve (Multi-Var Logistic)\")\n",
    "        plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "        plt.ylabel(\"True Positive Rate (Sensitivity)\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # If you want the \"optimal threshold\" from Youden's J statistic\n",
    "        optimal_threshold_index = np.argmax(tpr - fpr)\n",
    "        optimal_threshold = thresholds[optimal_threshold_index]\n",
    "        print(f\"Optimal Threshold (Youden's J) for multi-var logistic: {optimal_threshold:.3f}\")\n",
    "\n",
    "    def train_svm(self, drop_cols=None):\n",
    "        \"\"\"\n",
    "        Train a linear SVM on self.df for classification (pepper vs. no pepper),\n",
    "        then print classification report and plot ROC curve.\n",
    "        \"\"\"\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"Data not preprocessed yet. Call preprocess_data() first.\")\n",
    "\n",
    "        if drop_cols is None:\n",
    "            drop_cols = ['Received a pepper', 'Number of ratings', \n",
    "                         'Number of ratings coming from online classes',\n",
    "                         'Male Professor', 'Female Professor']\n",
    "\n",
    "        y = self.df['Received a pepper']\n",
    "        df_clean = self.df.drop(columns=drop_cols, errors='ignore')\n",
    "        X = df_clean.select_dtypes(include=[np.number])\n",
    "\n",
    "        # Scale\n",
    "        scaler = MinMaxScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_scaled, y, test_size=0.2, random_state=self.seed\n",
    "        )\n",
    "\n",
    "        svm_model = SVC(kernel='linear', probability=True, random_state=self.seed)\n",
    "        svm_model.fit(X_train, y_train)\n",
    "        self.svm_model = svm_model  # store model\n",
    "\n",
    "        y_pred = svm_model.predict(X_test)\n",
    "        y_prob = svm_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        print(\"Classification Report (SVM):\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        # Coefficients\n",
    "        w = svm_model.coef_[0]\n",
    "        b = svm_model.intercept_[0]\n",
    "        print(\"\\nSVM Coefficients (w):\", w)\n",
    "        print(\"SVM Intercept (b):\", b)\n",
    "\n",
    "        # ROC curve\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "        plt.title(\"ROC Curve (SVM)\")\n",
    "        plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "        plt.ylabel(\"True Positive Rate (Sensitivity)\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Data Reading & Preparation\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "# Read CSV file and assign column names\n",
    "df_capstone = pd.read_csv('./rmpCapstoneNum.csv', header=None)\n",
    "df_capstone.columns = [\n",
    "    'Average Rating',\n",
    "    'Average Difficulty',\n",
    "    'Number of ratings',\n",
    "    'Received a pepper',\n",
    "    'Proportion of students that said they would take the class again',\n",
    "    'Number of ratings coming from online classes',\n",
    "    'Male Professor',\n",
    "    'Female Professor'\n",
    "]\n",
    "\n",
    "# Filter out rows with fewer than 10 ratings\n",
    "df_capstone = df_capstone[df_capstone['Number of ratings'] >= 10]\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Missing-Value Inspection\n",
    "# -----------------------------\n",
    "\n",
    "print(\"Missing values per column:\\n\", df_capstone.isna().sum())\n",
    "\n",
    "# Split data into rows with missing values and rows without missing values\n",
    "df_no_na = df_capstone.dropna().copy()\n",
    "df_missing = df_capstone[df_capstone.isnull().any(axis=1)].copy()\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Descriptive Statistics\n",
    "# -----------------------------\n",
    "\n",
    "print(\"\\n--- Descriptive Statistics (No Missing) ---\")\n",
    "print(f\"Mean:   {df_no_na['Average Rating'].mean():.3f}\")\n",
    "print(f\"Median: {df_no_na['Average Rating'].median():.3f}\")\n",
    "print(f\"Std:    {df_no_na['Average Rating'].std():.3f}\")\n",
    "\n",
    "print(\"\\n--- Descriptive Statistics (Missing) ---\")\n",
    "print(f\"Mean:   {df_missing['Average Rating'].mean():.3f}\")\n",
    "print(f\"Median: {df_missing['Average Rating'].median():.3f}\")\n",
    "print(f\"Std:    {df_missing['Average Rating'].std():.3f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Boxplots Comparing Groups\n",
    "# -----------------------------\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(x=df_missing['Average Rating'], color='red')\n",
    "plt.title('Average Rating if Proportion column is missing')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(x=df_no_na['Average Rating'], color='blue')\n",
    "plt.title('Average Rating if Proportion column is present')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Histogram Comparison\n",
    "# -----------------------------\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(df_no_na['Average Rating'], \n",
    "             bins=30, kde=True, color='blue', \n",
    "             label='Proportion not missing', \n",
    "             stat='density')\n",
    "\n",
    "sns.histplot(df_missing['Average Rating'], \n",
    "             bins=30, kde=True, color='red', \n",
    "             label='Proportion missing', \n",
    "             stat='density')\n",
    "\n",
    "plt.xlabel('Average Professor Rating')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.title('Average Ratings by Missing vs. Non-Missing Proportion')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Two-Sample Tests\n",
    "# -----------------------------\n",
    "\n",
    "ks_stat, ks_p_val = stats.ks_2samp(df_no_na['Average Rating'], df_missing['Average Rating'])\n",
    "mw_stat, mw_p_val = stats.mannwhitneyu(df_no_na['Average Rating'], df_missing['Average Rating'])\n",
    "\n",
    "print(\"\\n--- Two-Sample Tests (Average Rating) ---\")\n",
    "print(f\"KS Test:           Statistic={ks_stat:.3f}, P-value={ks_p_val:.3f}\")\n",
    "print(f\"Mann-Whitney Test: Statistic={mw_stat:.3f}, P-value={mw_p_val:.3f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Summary Stats of No-NA Data\n",
    "# -----------------------------\n",
    "\n",
    "print(\"\\n--- Summary Stats: df_no_na ---\")\n",
    "print(df_no_na.describe())\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Add Proportion of Online Class Ratings\n",
    "# -----------------------------\n",
    "\n",
    "df_no_na['Proportion of online class ratings'] = (\n",
    "    df_no_na['Number of ratings coming from online classes'] /\n",
    "    df_no_na['Number of ratings']\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 9. Correlation Matrix\n",
    "# -----------------------------\n",
    "\n",
    "correlation_matrix = df_no_na.corr()\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(correlation_matrix, cmap=\"RdBu_r\", annot=True)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 10. Final DataFrame Filtering\n",
    "# -----------------------------\n",
    "\n",
    "# Keep rows where only male OR female professor\n",
    "df_capstone_dropped_final = df_no_na[\n",
    "    ((df_no_na['Male Professor'] == 1) & (df_no_na['Female Professor'] == 0)) |\n",
    "    ((df_no_na['Male Professor'] == 0) & (df_no_na['Female Professor'] == 1))\n",
    "]\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(\"\\n--- Final Filtered DataFrame ---\")\n",
    "print(df_capstone_dropped_final.head())\n",
    "print(f\"Final DF Shape: {df_capstone_dropped_final.shape}\")\n",
    "\n",
    "dependent_var='Average Rating'\n",
    "\n",
    "plot_feature_vs_dependent(df_capstone, dependent_var)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_subsets = [\n",
    "    [\n",
    "        'Proportion of students that said they would take the class again'\n",
    "    ],\n",
    "    [\n",
    "        'Average Difficulty',\n",
    "        'Received a pepper',\n",
    "        'Proportion of students that said they would take the class again'\n",
    "    ],\n",
    "    [\n",
    "        'Average Difficulty',\n",
    "        'Number of ratings',\n",
    "        'Received a pepper',\n",
    "        'Female Professor',\n",
    "        'Proportion of online class ratings',\n",
    "        'Proportion of students that said they would take the class again'\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Your target column\n",
    "target_col = 'Average Rating'\n",
    "\n",
    "# We'll do a single train/test split once at the beginning\n",
    "seed = seed\n",
    "df_train, df_test = train_test_split(df_capstone_dropped_final, \n",
    "                                     test_size=0.2, \n",
    "                                     random_state=seed)\n",
    "\n",
    "# We'll store final results for each subset in a list or dictionary\n",
    "all_subsets_results = []\n",
    "\n",
    "for idx, subset_cols in enumerate(feature_subsets, start=1):\n",
    "    print(f\"--- Subset #{idx}: {subset_cols} ---\")\n",
    "    \n",
    "    # 1) Create X_train, y_train, X_test, y_test\n",
    "    X_train = df_train[subset_cols].to_numpy()\n",
    "    y_train = df_train[target_col].to_numpy()\n",
    "    X_test = df_test[subset_cols].to_numpy()\n",
    "    y_test = df_test[target_col].to_numpy()\n",
    "    \n",
    "    # 2) Instantiate RegressionAnalysis for THIS subset\n",
    "    alphas = np.array([0.00001, 0.0001, 0.001, 0.01, \n",
    "                       0.1, 1, 2, 5, 10, 20, 100, 1000, 2000, 10000])\n",
    "    \n",
    "    analysis = RegressionAnalysis(\n",
    "        X_train, y_train,\n",
    "        alphas=alphas,\n",
    "        seed=seed,\n",
    "        n_splits=5  # K-folds\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # 3) Cross-validate on the TRAIN set\n",
    "    analysis.cross_validate()\n",
    "    \n",
    "    analysis.plot_cv_rmse_r2()\n",
    "\n",
    "    # 4) Pick best alpha for Ridge and Lasso based on RMSE from CV\n",
    "    best_ridge_alpha, best_ridge_rmse = analysis.pick_best_alpha('Ridge', metric='RMSE')\n",
    "    best_lasso_alpha, best_lasso_rmse = analysis.pick_best_alpha('Lasso', metric='RMSE')\n",
    "    \n",
    "    print(f\"Best Ridge alpha (subset #{idx}): {best_ridge_alpha} with mean CV-RMSE={best_ridge_rmse:.3f}\")\n",
    "    print(f\"Best Lasso alpha (subset #{idx}): {best_lasso_alpha} with mean CV-RMSE={best_lasso_rmse:.3f}\")\n",
    "    \n",
    "    # 5) Train final models on the full TRAIN set, evaluate once on TEST\n",
    "    #    This will store results in analysis.results_test\n",
    "    analysis.finalize_and_evaluate(\n",
    "        X_train, y_train,\n",
    "        X_test,  y_test,\n",
    "        best_ridge_alpha,\n",
    "        best_lasso_alpha,\n",
    "        make_residual_plots=True  # Set to True if you want residual subplots\n",
    "    )\n",
    "    \n",
    "    final_test_df = analysis.get_test_results_df()\n",
    "    print(\"Final Test Results:\\n\", final_test_df)\n",
    "    \n",
    "    # 6) Optionally, plot coefficients for each final model\n",
    "    #    a) Normal\n",
    "    normal_row = final_test_df[final_test_df['Model'] == 'Normal'].iloc[0]\n",
    "    betas_normal = normal_row['Betas']\n",
    "    analysis.plot_coefs(\n",
    "        betas_normal[1:], \n",
    "        feature_names=subset_cols,\n",
    "        model_name=f'Normal (Subset #{idx})'\n",
    "    )\n",
    "    \n",
    "    #    b) Best Ridge\n",
    "    ridge_row = final_test_df[\n",
    "        (final_test_df['Model'] == 'Ridge') & \n",
    "        (final_test_df['Alpha'] == best_ridge_alpha)\n",
    "    ].iloc[0]\n",
    "    betas_ridge = ridge_row['Betas']\n",
    "    analysis.plot_coefs(\n",
    "        betas_ridge[1:], \n",
    "        feature_names=subset_cols,\n",
    "        model_name=f'Ridge (Subset #{idx})', \n",
    "        alpha=best_ridge_alpha\n",
    "    )\n",
    "    \n",
    "    #    c) Best Lasso\n",
    "    lasso_row = final_test_df[\n",
    "        (final_test_df['Model'] == 'Lasso') & \n",
    "        (final_test_df['Alpha'] == best_lasso_alpha)\n",
    "    ].iloc[0]\n",
    "    betas_lasso = lasso_row['Betas']\n",
    "    analysis.plot_coefs(\n",
    "        betas_lasso[1:], \n",
    "        feature_names=subset_cols,\n",
    "        model_name=f'Lasso (Subset #{idx})', \n",
    "        alpha=best_lasso_alpha\n",
    "    )\n",
    "    \n",
    "    # Save or store this subset's final results for later\n",
    "    all_subsets_results.append({\n",
    "        'subset_index': idx,\n",
    "        'subset_columns': subset_cols,\n",
    "        'cv_results': analysis.get_cv_results_df(),  # Cross-validation data\n",
    "        'test_results': final_test_df                # Final test results\n",
    "    })\n",
    "    \n",
    "    print(\"------------------------------------------------------\\n\")\n",
    "\n",
    "# After the loop, you can compare all_subsets_results across your feature subsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read tags CSV\n",
    "tagsdf = pd.read_csv('./rmpCapstoneTags.csv', header=None)\n",
    "\n",
    "# Join tags with df_capstone based on shared index\n",
    "Q8df = df_capstone[['Average Rating', 'Number of ratings']].join(tagsdf, how='inner')\n",
    "\n",
    "# Drop rows with missing values\n",
    "Q8df.dropna(inplace=True)\n",
    "\n",
    "# Convert tag counts to proportions by dividing each tag count by total ratings\n",
    "for col in Q8df.columns[2:]:\n",
    "    Q8df[col] = Q8df[col].div(Q8df['Number of ratings'])\n",
    "\n",
    "# Subset to rows with at least 10 ratings\n",
    "Q8dfgreater10 = Q8df[Q8df['Number of ratings'] >= 10]\n",
    "\n",
    "# Identify pairs of columns with correlation > 0.42\n",
    "corr_matrix = Q8dfgreater10.corr()\n",
    "corrgreater40 = corr_matrix[corr_matrix > 0.42]\n",
    "high_corr_pairs = [\n",
    "    pair for pair in list(corrgreater40[corrgreater40.notnull()].stack().index)\n",
    "    if pair[0] != pair[1]\n",
    "]\n",
    "print(\"Highly correlated pairs (corr > 0.42):\", high_corr_pairs)\n",
    "\n",
    "# Heatmap of correlations (excluding 'Number of ratings')\n",
    "correlation_matrix = Q8dfgreater10.drop(columns=['Number of ratings']).corr()\n",
    "plt.figure(figsize=(40, 40))\n",
    "sns.heatmap(correlation_matrix, cmap=\"RdBu_r\", annot=True)\n",
    "plt.title('Correlation Matrix (Tags Only)')\n",
    "plt.show()\n",
    "\n",
    "# Scatter plots of each feature vs. Average Rating in a single plot with subplots\n",
    "dependent_var = 'Average Rating'\n",
    "\n",
    "plot_feature_vs_dependent(Q8dfgreater10, dependent_var)\n",
    "\n",
    "# Prepare data for feature selection\n",
    "X = Q8dfgreater10.drop(columns=['Average Rating', 'Number of ratings'])\n",
    "y = Q8dfgreater10['Average Rating']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Forward feature selection (K-fold CV)\n",
    "selected_features_kfold, forward_results_kfold = forward_feature_selection_kfold(X_train, y_train, k=5)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "forward_results_kfold_df = pd.DataFrame(\n",
    "    forward_results_kfold,\n",
    "    columns=['Selected Features', 'Betas', 'Alpha', 'RMSE', 'R2']\n",
    ")\n",
    "\n",
    "# Optionally remove Betas for cleaner display\n",
    "forward_results_kfold_df_cleaned = forward_results_kfold_df.drop(columns='Betas')\n",
    "\n",
    "# Plot RMSE and R² vs number of features\n",
    "plot_forward_selection_results(forward_results_kfold_df_cleaned)\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "best_selected_features_kfold = forward_results_kfold_df.iloc[-1]['Selected Features']\n",
    "X_train_best_kfold = X_train[best_selected_features_kfold]\n",
    "X_test_best_kfold  = X_test[best_selected_features_kfold]\n",
    "\n",
    "final_model_kfold = LinearRegression()\n",
    "final_model_kfold.fit(X_train_best_kfold, y_train)\n",
    "y_pred_test_kfold = final_model_kfold.predict(X_test_best_kfold)\n",
    "\n",
    "final_rmse_kfold = np.sqrt(mean_squared_error(y_test, y_pred_test_kfold))\n",
    "final_r2_kfold   = r2_score(y_test, y_pred_test_kfold)\n",
    "\n",
    "print(\"\\n----- Final Model Evaluation on Test Set -----\")\n",
    "print(f\"Selected Features: {best_selected_features_kfold}\")\n",
    "print(f\"Final Model RMSE: {final_rmse_kfold:.4f}\")\n",
    "print(f\"Final Model R²:   {final_r2_kfold:.4f}\")\n",
    "\n",
    "forward_results_kfold_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_subsets = [\n",
    "    list(range(20)), #0 to 19 features\n",
    "    [2],#TODO Try sqrt, sq, cube, log etc,\n",
    "    [0, 2, 1, 15, 16, 12]\n",
    "]\n",
    "\n",
    "\n",
    "# Your target column\n",
    "target_col = 'Average Rating'\n",
    "\n",
    "# We'll do a single train/test split once at the beginning\n",
    "df_train, df_test = train_test_split(Q8dfgreater10, \n",
    "                                     test_size=0.2, \n",
    "                                     random_state=seed)\n",
    "\n",
    "# We'll store final results for each subset in a list or dictionary\n",
    "all_subsets_results = []\n",
    "\n",
    "for idx, subset_cols in enumerate(feature_subsets, start=1):\n",
    "    print(f\"--- Subset #{idx}: {subset_cols} ---\")\n",
    "    \n",
    "    # 1) Create X_train, y_train, X_test, y_test\n",
    "    X_train = df_train[subset_cols].to_numpy()\n",
    "    y_train = df_train[target_col].to_numpy()\n",
    "    X_test = df_test[subset_cols].to_numpy()\n",
    "    y_test = df_test[target_col].to_numpy()\n",
    "    \n",
    "    # 2) Instantiate RegressionAnalysis for THIS subset\n",
    "    alphas = np.array([0.00001, 0.0001, 0.001, 0.01, \n",
    "                       0.1, 1, 2, 5, 10, 20, 100, 1000, 2000, 10000])\n",
    "    \n",
    "    analysis = RegressionAnalysis(\n",
    "        X_train, y_train,\n",
    "        alphas=alphas,\n",
    "        seed=seed,\n",
    "        n_splits=5  # K-folds\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # 3) Cross-validate on the TRAIN set\n",
    "    analysis.cross_validate()\n",
    "    \n",
    "    analysis.plot_cv_rmse_r2()\n",
    "\n",
    "    # 4) Pick best alpha for Ridge and Lasso based on RMSE from CV\n",
    "    best_ridge_alpha, best_ridge_rmse = analysis.pick_best_alpha('Ridge', metric='RMSE')\n",
    "    best_lasso_alpha, best_lasso_rmse = analysis.pick_best_alpha('Lasso', metric='RMSE')\n",
    "    \n",
    "    print(f\"Best Ridge alpha (subset #{idx}): {best_ridge_alpha} with mean CV-RMSE={best_ridge_rmse:.3f}\")\n",
    "    print(f\"Best Lasso alpha (subset #{idx}): {best_lasso_alpha} with mean CV-RMSE={best_lasso_rmse:.3f}\")\n",
    "    \n",
    "    # 5) Train final models on the full TRAIN set, evaluate once on TEST\n",
    "    #    This will store results in analysis.results_test\n",
    "    analysis.finalize_and_evaluate(\n",
    "        X_train, y_train,\n",
    "        X_test,  y_test,\n",
    "        best_ridge_alpha,\n",
    "        best_lasso_alpha,\n",
    "        make_residual_plots=True  # Set to True if you want residual subplots\n",
    "    )\n",
    "    \n",
    "    final_test_df = analysis.get_test_results_df()\n",
    "    print(\"Final Test Results:\\n\", final_test_df)\n",
    "    \n",
    "    # 6) Optionally, plot coefficients for each final model\n",
    "    #    a) Normal\n",
    "    normal_row = final_test_df[final_test_df['Model'] == 'Normal'].iloc[0]\n",
    "    betas_normal = normal_row['Betas']\n",
    "    analysis.plot_coefs(\n",
    "        betas_normal[1:], \n",
    "        feature_names=subset_cols,\n",
    "        model_name=f'Normal (Subset #{idx})'\n",
    "    )\n",
    "    \n",
    "    #    b) Best Ridge\n",
    "    ridge_row = final_test_df[\n",
    "        (final_test_df['Model'] == 'Ridge') & \n",
    "        (final_test_df['Alpha'] == best_ridge_alpha)\n",
    "    ].iloc[0]\n",
    "    betas_ridge = ridge_row['Betas']\n",
    "    analysis.plot_coefs(\n",
    "        betas_ridge[1:], \n",
    "        feature_names=subset_cols,\n",
    "        model_name=f'Ridge (Subset #{idx})', \n",
    "        alpha=best_ridge_alpha\n",
    "    )\n",
    "    \n",
    "    #    c) Best Lasso\n",
    "    lasso_row = final_test_df[\n",
    "        (final_test_df['Model'] == 'Lasso') & \n",
    "        (final_test_df['Alpha'] == best_lasso_alpha)\n",
    "    ].iloc[0]\n",
    "    betas_lasso = lasso_row['Betas']\n",
    "    analysis.plot_coefs(\n",
    "        betas_lasso[1:], \n",
    "        feature_names=subset_cols,\n",
    "        model_name=f'Lasso (Subset #{idx})', \n",
    "        alpha=best_lasso_alpha\n",
    "    )\n",
    "    \n",
    "    # Save or store this subset's final results for later\n",
    "    all_subsets_results.append({\n",
    "        'subset_index': idx,\n",
    "        'subset_columns': subset_cols,\n",
    "        'cv_results': analysis.get_cv_results_df(),  # Cross-validation data\n",
    "        'test_results': final_test_df                # Final test results\n",
    "    })\n",
    "    \n",
    "    print(\"------------------------------------------------------\\n\")\n",
    "\n",
    "# After the loop, you can compare all_subsets_results across your feature subsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "Q9df = df_capstone[['Average Difficulty', 'Number of ratings']].join(tagsdf, how='inner')\n",
    "Q9df.dropna(inplace=True)\n",
    "\n",
    "# Normalize tags by 'Number of ratings'\n",
    "for column in Q9df.columns[2:]:\n",
    "    Q9df[column] = Q9df[column].div(Q9df['Number of ratings'])\n",
    "\n",
    "# Filter rows with 'Number of ratings' >= 10\n",
    "Q9dfgreater10 = Q9df[Q9df['Number of ratings'] >= 10]\n",
    "\n",
    "# Correlation analysis\n",
    "correlation_matrix = Q9dfgreater10.drop(columns=['Number of ratings']).corr()\n",
    "plt.figure(figsize=(40, 40))\n",
    "sns.heatmap(correlation_matrix, cmap=\"RdBu_r\", annot=True)\n",
    "plt.title('Correlation Matrix (Tags Only)')\n",
    "plt.show()\n",
    "\n",
    "# Generate scatter plots of features vs. 'Average Difficulty'\n",
    "dependent_var = 'Average Difficulty'\n",
    "plot_feature_vs_dependent(Q9dfgreater10, dependent_var)\n",
    "\n",
    "# Prepare data for modeling\n",
    "X = Q9dfgreater10.drop(columns=['Average Difficulty', 'Number of ratings'])\n",
    "y = Q9dfgreater10['Average Difficulty']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Forward feature selection with K-fold cross-validation\n",
    "selected_features_kfold, forward_results_kfold = forward_feature_selection_kfold(X_train, y_train, k=5)\n",
    "\n",
    "# Convert results to DataFrame for analysis\n",
    "forward_results_kfold_df = pd.DataFrame(\n",
    "    forward_results_kfold,\n",
    "    columns=['Selected Features', 'Betas', 'Alpha', 'RMSE', 'R2']\n",
    ")\n",
    "\n",
    "# Optional: Clean up the results by removing the 'Betas' column\n",
    "forward_results_kfold_df_cleaned = forward_results_kfold_df.drop(columns='Betas')\n",
    "\n",
    "# Plot RMSE and R² vs number of features\n",
    "plot_forward_selection_results(forward_results_kfold_df_cleaned)\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "best_selected_features_kfold = forward_results_kfold_df.iloc[-1]['Selected Features']\n",
    "X_train_best_kfold = X_train[best_selected_features_kfold]\n",
    "X_test_best_kfold = X_test[best_selected_features_kfold]\n",
    "\n",
    "final_model_kfold = LinearRegression()\n",
    "final_model_kfold.fit(X_train_best_kfold, y_train)\n",
    "y_pred_test_kfold = final_model_kfold.predict(X_test_best_kfold)\n",
    "\n",
    "# Calculate final metrics\n",
    "final_rmse_kfold = np.sqrt(mean_squared_error(y_test, y_pred_test_kfold))\n",
    "final_r2_kfold = r2_score(y_test, y_pred_test_kfold)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n----- Final Model Evaluation on Test Set -----\")\n",
    "print(f\"Selected Features: {best_selected_features_kfold}\")\n",
    "print(f\"Final Model RMSE: {final_rmse_kfold:.4f}\")\n",
    "print(f\"Final Model R²:   {final_r2_kfold:.4f}\")\n",
    "\n",
    "# Display feature selection results\n",
    "forward_results_kfold_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_subsets = [\n",
    "    list(range(20)), #0 to 19 features\n",
    "    [0],#TODO Try sqrt, sq, cube etc,\n",
    "    [0, 13, 6, 3, 9]\n",
    "]\n",
    "\n",
    "\n",
    "# Your target column\n",
    "target_col = 'Average Difficulty'\n",
    "\n",
    "# We'll do a single train/test split once at the beginning\n",
    "df_train, df_test = train_test_split(Q9dfgreater10, \n",
    "                                     test_size=0.2, \n",
    "                                     random_state=seed)\n",
    "\n",
    "# We'll store final results for each subset in a list or dictionary\n",
    "all_subsets_results = []\n",
    "\n",
    "for idx, subset_cols in enumerate(feature_subsets, start=1):\n",
    "    print(f\"--- Subset #{idx}: {subset_cols} ---\")\n",
    "    \n",
    "    # 1) Create X_train, y_train, X_test, y_test\n",
    "    X_train = df_train[subset_cols].to_numpy()\n",
    "    y_train = df_train[target_col].to_numpy()\n",
    "    X_test = df_test[subset_cols].to_numpy()\n",
    "    y_test = df_test[target_col].to_numpy()\n",
    "    \n",
    "    # 2) Instantiate RegressionAnalysis for THIS subset\n",
    "    alphas = np.array([0.00001, 0.0001, 0.001, 0.01, \n",
    "                       0.1, 1, 2, 5, 10, 20, 100, 1000, 2000, 10000])\n",
    "    \n",
    "    analysis = RegressionAnalysis(\n",
    "        X_train, y_train,\n",
    "        alphas=alphas,\n",
    "        seed=seed,\n",
    "        n_splits=5  # K-folds\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # 3) Cross-validate on the TRAIN set\n",
    "    analysis.cross_validate()\n",
    "    \n",
    "    analysis.plot_cv_rmse_r2()\n",
    "\n",
    "    # 4) Pick best alpha for Ridge and Lasso based on RMSE from CV\n",
    "    best_ridge_alpha, best_ridge_rmse = analysis.pick_best_alpha('Ridge', metric='RMSE')\n",
    "    best_lasso_alpha, best_lasso_rmse = analysis.pick_best_alpha('Lasso', metric='RMSE')\n",
    "    \n",
    "    print(f\"Best Ridge alpha (subset #{idx}): {best_ridge_alpha} with mean CV-RMSE={best_ridge_rmse:.3f}\")\n",
    "    print(f\"Best Lasso alpha (subset #{idx}): {best_lasso_alpha} with mean CV-RMSE={best_lasso_rmse:.3f}\")\n",
    "    \n",
    "    # 5) Train final models on the full TRAIN set, evaluate once on TEST\n",
    "    #    This will store results in analysis.results_test\n",
    "    analysis.finalize_and_evaluate(\n",
    "        X_train, y_train,\n",
    "        X_test,  y_test,\n",
    "        best_ridge_alpha,\n",
    "        best_lasso_alpha,\n",
    "        make_residual_plots=True  # Set to True if you want residual subplots\n",
    "    )\n",
    "    \n",
    "    final_test_df = analysis.get_test_results_df()\n",
    "    print(\"Final Test Results:\\n\", final_test_df)\n",
    "    \n",
    "    # 6) Optionally, plot coefficients for each final model\n",
    "    #    a) Normal\n",
    "    normal_row = final_test_df[final_test_df['Model'] == 'Normal'].iloc[0]\n",
    "    betas_normal = normal_row['Betas']\n",
    "    analysis.plot_coefs(\n",
    "        betas_normal[1:], \n",
    "        feature_names=subset_cols,\n",
    "        model_name=f'Normal (Subset #{idx})'\n",
    "    )\n",
    "    \n",
    "    #    b) Best Ridge\n",
    "    ridge_row = final_test_df[\n",
    "        (final_test_df['Model'] == 'Ridge') & \n",
    "        (final_test_df['Alpha'] == best_ridge_alpha)\n",
    "    ].iloc[0]\n",
    "    betas_ridge = ridge_row['Betas']\n",
    "    analysis.plot_coefs(\n",
    "        betas_ridge[1:], \n",
    "        feature_names=subset_cols,\n",
    "        model_name=f'Ridge (Subset #{idx})', \n",
    "        alpha=best_ridge_alpha\n",
    "    )\n",
    "    \n",
    "    #    c) Best Lasso\n",
    "    lasso_row = final_test_df[\n",
    "        (final_test_df['Model'] == 'Lasso') & \n",
    "        (final_test_df['Alpha'] == best_lasso_alpha)\n",
    "    ].iloc[0]\n",
    "    betas_lasso = lasso_row['Betas']\n",
    "    analysis.plot_coefs(\n",
    "        betas_lasso[1:], \n",
    "        feature_names=subset_cols,\n",
    "        model_name=f'Lasso (Subset #{idx})', \n",
    "        alpha=best_lasso_alpha\n",
    "    )\n",
    "    \n",
    "    # Save or store this subset's final results for later\n",
    "    all_subsets_results.append({\n",
    "        'subset_index': idx,\n",
    "        'subset_columns': subset_cols,\n",
    "        'cv_results': analysis.get_cv_results_df(),  # Cross-validation data\n",
    "        'test_results': final_test_df                # Final test results\n",
    "    })\n",
    "    \n",
    "    print(\"------------------------------------------------------\\n\")\n",
    "\n",
    "# After the loop, you can compare all_subsets_results across your feature subsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Instantiate the class with your dataframes (df_capstone, tagsdf)\n",
    "#    Suppose df_capstone_dropped_final = ... and tagsdf = ...\n",
    "analysis = PepperAnalysis(df_capstone, tagsdf, seed=seed)\n",
    "\n",
    "# 2. Preprocess data (merge, drop NAs, compute proportions, filter for male/female, etc.)\n",
    "analysis.preprocess_data()\n",
    "\n",
    "# 3. (Optional) Plot correlation matrix to see the big picture\n",
    "analysis.plot_correlation_matrix()\n",
    "\n",
    "# 4. (Optional) Plot scatter for single variable\n",
    "analysis.plot_scatter_single(x_col='Average Rating', y_col='Received a pepper')\n",
    "\n",
    "# 5. Single-variable logistic regression\n",
    "#    You can supply a threshold if you want something other than 0.5\n",
    "analysis.logistic_regression_single_var(\n",
    "    x_col='Average Rating', \n",
    "    y_col='Received a pepper', \n",
    "    threshold=0.607  # or pick from ROC\n",
    ")\n",
    "\n",
    "# 6. Multi-variable logistic regression (by default, it will drop certain columns)\n",
    "#    Adjust threshold as needed, or see the \"optimal\" from the ROC curve\n",
    "analysis.logistic_regression_multi_var(threshold=0.465)\n",
    "\n",
    "# 7. Train a linear SVM for comparison\n",
    "analysis.train_svm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
