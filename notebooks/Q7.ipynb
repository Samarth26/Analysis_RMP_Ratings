{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activists have asserted that there is a strong gender bias in student evaluations of professors, with \n",
    "male professors enjoying a boost in rating from this bias. While this has been celebrated by ideologues, \n",
    "skeptics have pointed out that this research is of technically poor quality, either due to a low sample \n",
    "size – as small as n = 1 (Mitchell & Martin, 2018), failure to control for confounders such as teaching \n",
    "experience (Centra & Gaubatz, 2000) or obvious p-hacking (MacNell et al., 2015). We would like you to \n",
    "answer the question whether there is evidence of a pro-male gender bias in this dataset.  \n",
    "Hint: A significance test is probably required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns represent the following information, in order:\n",
    "1. Average Rating (the arithmetic mean of all individual quality ratings of this professor)\n",
    "2. Average Difficulty (the arithmetic mean of all individual difficulty ratings of this professor)\n",
    "3. Number of ratings (simply the total number of ratings these averages are based on)\n",
    "4. Received a “pepper”? (Boolean - was this professor judged as “hot” by the students?)\n",
    "5. The proportion of students that said they would take the class again\n",
    "6. The number of ratings coming from online classes\n",
    "7. Male gender (Boolean – 1: determined with high confidence that professor is male)\n",
    "8. Female (Boolean – 1: determined with high confidence that professor is female)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import KFold  \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "seed = 10676128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Build a regression model predicting average rating from all numerical predictors (the ones in the rmpCapstoneNum.csv) file. Make sure to include the R2 and RMSE of this model. Which of these factors is most strongly predictive of average rating? Hint: Make sure to address collinearity concerns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_capstone = pd.read_csv('./rmpCapstoneNum.csv', header=None)\n",
    "df_capstone.columns = ['Average Rating', 'Average Difficulty', 'Number of ratings', 'Received a pepper', \n",
    "                       'Proportion of students that said they would take the class again', \n",
    "                       'Number of ratings coming from online classes', 'Male Professor', 'Female Professor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_capstone=df_capstone[df_capstone['Number of ratings'] >= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_capstone.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_capstone_na_dropped=df_capstone.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_capstone_na_dropped['Average Rating'].mean(),df_capstone_na_dropped['Average Rating'].median(),df_capstone_na_dropped['Average Rating'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_only_na=df_capstone[df_capstone.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_only_na['Average Rating'].mean(),df_only_na['Average Rating'].median(),df_only_na['Average Rating'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df_only_na['Average Rating'])\n",
    "plt.title('Distribution of Average Rating if Proportion column is missing')\n",
    "plt.show()\n",
    "sns.boxplot(df_capstone_na_dropped['Average Rating'])\n",
    "plt.title('Distribution of Average Rating if Proportion column is present')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.histplot(df_capstone_na_dropped['Average Rating'], bins=30, kde=True, color='blue', label='Average Rating if prop not missing', stat='density')\n",
    "sns.histplot(df_only_na['Average Rating'], bins=30, kde=True, color='red', label='Average Rating if prop missing', stat='density')\n",
    "\n",
    "plt.title('')\n",
    "plt.xlabel('AverageProfessorRating')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_stat, p_val = stats.ks_2samp(df_capstone_na_dropped['Average Rating'], df_only_na['Average Rating'])\n",
    "mannwhitney_stat, mannwhitney_p_val = stats.mannwhitneyu(df_capstone_na_dropped['Average Rating'], df_only_na['Average Rating'])\n",
    "\n",
    "print('KS Statistic: ', ks_stat)\n",
    "print('P-value: ', p_val)\n",
    "print('Mann Whitney U Statistic: ', mannwhitney_stat)\n",
    "print('Mann Whitney U P-value: ', mannwhitney_p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_capstone_na_dropped.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_capstone_na_dropped['Proportion of online class ratings'] = df_capstone_na_dropped['Number of ratings coming from online classes'].div(df_capstone_na_dropped['Number of ratings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_capstone_na_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df_capstone_na_dropped.corr()\n",
    "\n",
    "sns.heatmap(correlation_matrix,cmap = \"RdBu_r\", annot=True)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_capstone_dropped_final=df_capstone_na_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_capstone_dropped_final=df_capstone_dropped_final[(df_capstone_dropped_final['Male Professor']==1) & (df_capstone_dropped_final['Female Professor']==0) | (df_capstone_dropped_final['Male Professor']==0) & (df_capstone_dropped_final['Female Professor']==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_capstone_dropped_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "def compute_r2(y_true, y_pred):\n",
    "    ss_total = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    ss_residual = np.sum((y_true - y_pred) ** 2)\n",
    "    return 1 - (ss_residual / ss_total)\n",
    "\n",
    "def normal_regression(X_train, y_train):\n",
    "    return np.linalg.inv(X_train.T @ X_train) @ X_train.T @ y_train\n",
    "\n",
    "def ridge_regression(X_train, y_train, alpha):\n",
    "    n_features = X_train.shape[1]\n",
    "    identity = np.eye(n_features)\n",
    "    identity[0, 0] = 0  \n",
    "    return np.linalg.inv(X_train.T @ X_train + alpha * identity) @ X_train.T @ y_train\n",
    "\n",
    "def lasso_regression(X_train, y_train, alpha, max_iter=10000, tol=1e-6):\n",
    "    m, n = X_train.shape\n",
    "    beta = np.zeros(n)\n",
    "    for _ in range(max_iter):\n",
    "        beta_old = beta.copy()\n",
    "        for j in range(n):\n",
    "            residual = y_train - X_train @ beta + X_train[:, j] * beta[j]\n",
    "            rho = X_train[:, j].T @ residual\n",
    "            if j == 0:  \n",
    "                beta[j] = rho / (X_train[:, j].T @ X_train[:, j])\n",
    "            else:\n",
    "                beta[j] = np.sign(rho) * max(0, abs(rho) - alpha) / (X_train[:, j].T @ X_train[:, j])\n",
    "        if np.max(np.abs(beta - beta_old)) < tol:\n",
    "            break\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKFresults(X,y,alphas=np.array([0.00001,0.0001,0.001,0.01,0.1,1,2,5,10,20,100,1000,2000,100000])):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    results = []\n",
    "\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        X_train_scaled = np.hstack((np.ones((X_train_scaled.shape[0], 1)), X_train_scaled))\n",
    "        X_val_scaled = np.hstack((np.ones((X_val_scaled.shape[0], 1)), X_val_scaled))\n",
    "\n",
    "        # Normal Regression\n",
    "        beta_normal = normal_regression(X_train_scaled, y_train)\n",
    "        y_pred_normal = X_val_scaled @ beta_normal\n",
    "        rmse_normal = compute_rmse(y_val, y_pred_normal)\n",
    "        r2_normal = compute_r2(y_val, y_pred_normal)\n",
    "        \n",
    "        # Ridge Regression\n",
    "        for alpha in alphas:\n",
    "            beta_ridge = ridge_regression(X_train_scaled, y_train, alpha)\n",
    "            y_pred_ridge = X_val_scaled @ beta_ridge\n",
    "            rmse_ridge = compute_rmse(y_val, y_pred_ridge)\n",
    "            r2_ridge = compute_r2(y_val, y_pred_ridge)\n",
    "            results.append(('Ridge', beta_ridge,alpha, rmse_ridge, r2_ridge))\n",
    "        \n",
    "        # Lasso Regression\n",
    "        for alpha in alphas:\n",
    "            beta_lasso = lasso_regression(X_train_scaled, y_train, alpha)\n",
    "            y_pred_lasso = X_val_scaled @ beta_lasso\n",
    "            rmse_lasso = compute_rmse(y_val, y_pred_lasso)\n",
    "            r2_lasso = compute_r2(y_val, y_pred_lasso)\n",
    "            results.append(('Lasso', beta_lasso, alpha, rmse_lasso, r2_lasso))\n",
    "        \n",
    "        results.append(('Normal',beta_normal, None, rmse_normal, r2_normal))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getfinalresults(X_train,y_train,X_test,y_test,alphas=np.array([0.00001,0.0001,0.001,0.01,0.1,1,2,5,10,20,100,1000,2000,100000])):\n",
    "    results = []\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_train_scaled = np.hstack((np.ones((X_train_scaled.shape[0], 1)), X_train_scaled))\n",
    "    X_test_scaled = np.hstack((np.ones((X_test_scaled.shape[0], 1)), X_test_scaled))\n",
    "\n",
    "    # Normal Regression\n",
    "    beta_normal = normal_regression(X_train_scaled, y_train)\n",
    "    y_pred_normal = X_test_scaled @ beta_normal\n",
    "    rmse_normal = compute_rmse(y_test, y_pred_normal)\n",
    "    r2_normal = compute_r2(y_test, y_pred_normal)\n",
    "\n",
    "    residuals = y_test - y_pred_normal\n",
    "\n",
    "    ## Residual plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Grab standardized residuals for plot\n",
    "    std_residuals = (residuals - residuals.mean()) / residuals.std()\n",
    "    plt.scatter(y_pred_normal, std_residuals, color='blue')\n",
    "    plt.axhline(y=0, color='red', linestyle='--', linewidth=1)  # Horizontal line at y=0\n",
    "\n",
    "    # Labels and title\n",
    "    plt.title(\"Residual Plot\")\n",
    "    plt.xlabel(\"Predicted Values (y_hat)\")\n",
    "    plt.ylabel(\"Standardized Residuals\")\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    ## Pretty even spread in the residuals (homoscedasticity? - check)\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "    # See histogram of residuals to check for normality\n",
    "    ax.hist(residuals, bins=15, color='green', edgecolor='black', density=True) # Add density\n",
    "    ax.set_title(\"Histogram of Residuals\")\n",
    "    ax.set_xlabel(\"Residuals\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    \n",
    "    # Ridge Regression\n",
    "    for alpha in alphas:\n",
    "        beta_ridge = ridge_regression(X_train_scaled, y_train, alpha)\n",
    "        y_pred_ridge = X_test_scaled @ beta_ridge\n",
    "        rmse_ridge = compute_rmse(y_test, y_pred_ridge)\n",
    "        r2_ridge = compute_r2(y_test, y_pred_ridge)\n",
    "        results.append(('Ridge', alpha, rmse_ridge, r2_ridge))\n",
    "    \n",
    "    # Lasso Regression\n",
    "    for alpha in alphas:\n",
    "        beta_lasso = lasso_regression(X_train_scaled, y_train, alpha)\n",
    "        y_pred_lasso = X_test_scaled @ beta_lasso\n",
    "        rmse_lasso = compute_rmse(y_test, y_pred_lasso)\n",
    "        r2_lasso = compute_r2(y_test, y_pred_lasso)\n",
    "        results.append(('Lasso', alpha, rmse_lasso, r2_lasso))\n",
    "        residuals = y_test - y_pred_lasso\n",
    "\n",
    "        ## Residual plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        # Grab standardized residuals for plot\n",
    "        std_residuals = (residuals - residuals.mean()) / residuals.std()\n",
    "        plt.scatter(y_pred_lasso, std_residuals, color='blue')\n",
    "        plt.axhline(y=0, color='red', linestyle='--', linewidth=1)  # Horizontal line at y=0\n",
    "\n",
    "        # Labels and title\n",
    "        plt.title(f\"Residual Plot lasso alpha= {alpha}\")\n",
    "        plt.xlabel(\"Predicted Values (y_hat)\")\n",
    "        plt.ylabel(\"Standardized Residuals\")\n",
    "\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        ## Pretty even spread in the residuals (homoscedasticity? - check)\n",
    "        fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "        # See histogram of residuals to check for normality\n",
    "        ax.hist(residuals, bins=15, color='green', edgecolor='black', density=True) # Add density\n",
    "        ax.set_title(\"Histogram of Residuals\")\n",
    "        ax.set_xlabel(\"Residuals\")\n",
    "        ax.set_ylabel(\"Density\")\n",
    "    \n",
    "    results.append(('Normal', None, rmse_normal, r2_normal))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getbetas(X_train,y_train,X_test,y_test):\n",
    "    results=[]\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_train_scaled = np.hstack((np.ones((X_train_scaled.shape[0], 1)), X_train_scaled))\n",
    "    X_test_scaled = np.hstack((np.ones((X_test_scaled.shape[0], 1)), X_test_scaled))\n",
    "\n",
    "    # Normal Regression\n",
    "    beta_normal = normal_regression(X_train_scaled, y_train)\n",
    "    y_pred_normal = X_test_scaled @ beta_normal\n",
    "    rmse_normal = compute_rmse(y_test, y_pred_normal)\n",
    "    r2_normal = compute_r2(y_test, y_pred_normal)\n",
    "    results.append(('Normal',beta_normal, rmse_normal, r2_normal))\n",
    "    return beta_normal,rmse_normal,r2_normal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getbetaslasso(X_train,y_train,X_test,y_test,alpha=10):\n",
    "    results=[]\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_train_scaled = np.hstack((np.ones((X_train_scaled.shape[0], 1)), X_train_scaled))\n",
    "    X_test_scaled = np.hstack((np.ones((X_test_scaled.shape[0], 1)), X_test_scaled))\n",
    "\n",
    "    # Normal Regression\n",
    "    beta_lasso = lasso_regression(X_train_scaled, y_train, alpha)\n",
    "    y_pred_lasso = X_test_scaled @ beta_lasso\n",
    "    rmse_lasso = compute_rmse(y_test, y_pred_lasso)\n",
    "    r2_lasso = compute_r2(y_test, y_pred_lasso)\n",
    "    results.append(('Lasso', beta_lasso, rmse_lasso, r2_lasso))\n",
    "    return beta_lasso,rmse_lasso,r2_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getbetasridge(X_train,y_train,X_test,y_test,alpha=10):\n",
    "    results=[]\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_train_scaled = np.hstack((np.ones((X_train_scaled.shape[0], 1)), X_train_scaled))\n",
    "    X_test_scaled = np.hstack((np.ones((X_test_scaled.shape[0], 1)), X_test_scaled))\n",
    "\n",
    "    # Normal Regression\n",
    "    beta_ridge = ridge_regression(X_train_scaled, y_train, alpha)\n",
    "    y_pred_ridge = X_test_scaled @ beta_ridge\n",
    "    rmse_ridge = compute_rmse(y_test, y_pred_ridge)\n",
    "    r2_ridge = compute_r2(y_test, y_pred_ridge)\n",
    "    results.append(('ridge', beta_ridge, rmse_ridge, r2_ridge))\n",
    "    return beta_ridge,rmse_ridge,r2_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results_df,alphas=np.array([0.00001,0.0001,0.001,0.01,0.1,1,2,5,10,20,100,1000,2000,100000])):\n",
    "    ridge_results = results_df[results_df['Model'] == 'Ridge'].drop(columns=['Model']).groupby('Alpha').mean()\n",
    "    lasso_results = results_df[results_df['Model'] == 'Lasso'].drop(columns=['Model']).groupby('Alpha').mean()\n",
    "    normal_results = results_df[results_df['Model']=='Normal'].drop(columns=['Model','Alpha']).mean()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.xscale(\"log\")\n",
    "\n",
    "    plt.plot(alphas, ridge_results['RMSE'], marker='o', label='Ridge Regression RMSE')\n",
    "    plt.plot(alphas, lasso_results['RMSE'], marker='s', label='Lasso Regression RMSE')\n",
    "    \n",
    "    plt.axhline(y=normal_results['RMSE'], color='r', linestyle='--', label='Normal Regression RMSE')\n",
    "\n",
    "    plt.title('RMSE Comparison Across Models')\n",
    "    plt.xlabel('Alpha (Regularization Parameter)')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.xscale(\"log\")\n",
    "\n",
    "    plt.plot(alphas, ridge_results['R2'], marker='o', label='Ridge Regression R2')\n",
    "    plt.plot(alphas, lasso_results['R2'], marker='s', label='Lasso Regression R2')\n",
    "\n",
    "    plt.axhline(y=normal_results['R2'], color='r', linestyle='--', label='Normal Regression R2')\n",
    "\n",
    "    plt.title('R2 Comparison Across Models')\n",
    "    plt.xlabel('Alpha (Regularization Parameter)')\n",
    "    plt.ylabel('R2')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_betas(betas,feature_names=0,mytype='Normal',alpha=10):\n",
    "    # Plot the coefficients (betas) for all models\n",
    "    plt.figure(figsize=(24, 6))\n",
    "    # Ridge Regression Coefficients (for best alpha)\n",
    "    if(feature_names==0):\n",
    "        feature_names=range(len(betas))\n",
    "    #plt.xticks(range(len(betas)))\n",
    "    plt.bar([str(i) for i in feature_names], betas)\n",
    "    if mytype=='Normal':\n",
    "        plt.title(f'Coefficients Regression)')\n",
    "    if mytype=='Ridge':\n",
    "        plt.title(f'Coefficients Ridge (alpha={alpha})')\n",
    "    if mytype=='Lasso':\n",
    "        plt.title(f'Coefficients Ridge (alpha={alpha})')\n",
    "    plt.xlabel('Feature Index')\n",
    "    plt.ylabel('Coefficient Value')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent_var='Average Rating'\n",
    "for var in df_capstone_dropped_final.columns:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.scatterplot(x=df_capstone_dropped_final[var], y=df_capstone_dropped_final[dependent_var])\n",
    "    plt.title(f'Scatterplot of {dependent_var} vs {var}')\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel(dependent_var)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    df_capstone_dropped_final['Average Difficulty'],\n",
    "    df_capstone_dropped_final['Number of ratings'],\n",
    "    df_capstone_dropped_final['Received a pepper'],\n",
    "    df_capstone_dropped_final['Female Professor'],\n",
    "    df_capstone_dropped_final['Proportion of online class ratings'],\n",
    "    df_capstone_dropped_final['Proportion of students that said they would take the class again']\n",
    "\n",
    "]).T\n",
    "\n",
    "y = np.array(df_capstone_dropped_final['Average Rating'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "results=getKFresults(X,y,alphas=np.array([0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 2, 5, 10, 20, 100, 1000,1500, 100000]))\n",
    "\n",
    "results_df2 = pd.DataFrame(results, columns=['Model','Betas','Alpha', 'RMSE', 'R2'])\n",
    "results_df = pd.DataFrame(results, columns=['Model','Betas','Alpha', 'RMSE', 'R2']).drop(columns='Betas')\n",
    "\n",
    "plot_results(results_df,alphas=np.array([0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 2, 5, 10, 20, 100, 1000,1500, 100000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i,j) for i,j in zip(results_df2[results_df2['Model']=='Lasso']['Betas'][0:14],results_df2[results_df2['Model']=='Lasso']['RMSE'][0:14])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[results_df['Model'] == 'Ridge'].drop(columns=['Model']).groupby('Alpha').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=getfinalresults(X_train,y_train,X_test,y_test,alphas=np.array([0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 2, 5, 10, 20, 100, 1000,2000, 100000]))\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'Alpha', 'RMSE', 'R2'])\n",
    "plot_results(results_df,alphas=np.array([0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 2, 5, 10, 20, 100, 1000,2000, 100000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mybetas=getbetas(X_train,y_train,X_test,y_test)[0]\n",
    "indices = np.argsort(mybetas)[::-1]\n",
    "print(indices)  \n",
    "print(getbetas(X_train,y_train,X_test,y_test))\n",
    "plot_betas(getbetas(X_train,y_train,X_test,y_test)[0][1:],[i.name for i in [\n",
    "    df_capstone_dropped_final['Average Difficulty'],\n",
    "    df_capstone_dropped_final['Number of ratings'],\n",
    "    df_capstone_dropped_final['Received a pepper'],\n",
    "    df_capstone_dropped_final['Female Professor'],\n",
    "    df_capstone_dropped_final['Proportion of online class ratings'],\n",
    "    df_capstone_dropped_final['Proportion of students that said they would take the class again']\n",
    "]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    df_capstone_dropped_final['Proportion of students that said they would take the class again'],\n",
    "    df_capstone_dropped_final['Average Difficulty'],\n",
    "    df_capstone_dropped_final['Received a pepper']\n",
    "]).T\n",
    "\n",
    "y = np.array(df_capstone_dropped_final['Average Rating'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "results=getKFresults(X_train,y_train)\n",
    "\n",
    "results_df2 = pd.DataFrame(results, columns=['Model','Betas','Alpha', 'RMSE', 'R2'])\n",
    "results_df = pd.DataFrame(results, columns=['Model','Betas','Alpha', 'RMSE', 'R2']).drop(columns='Betas')\n",
    "plot_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mybetas=getbetas(X_train,y_train,X_test,y_test)[0] \n",
    "indices = np.argsort(mybetas)[::-1] \n",
    "print(indices)  \n",
    "print(getbetas(X_train,y_train,X_test,y_test))\n",
    "plot_betas(getbetas(X_train,y_train,X_test,y_test)[0][1:],[i.name for i in [\n",
    "    df_capstone_dropped_final['Proportion of students that said they would take the class again'],\n",
    "    df_capstone_dropped_final['Average Difficulty'],\n",
    "    df_capstone_dropped_final['Received a pepper']\n",
    "]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in results_df2[results_df2['Model']=='Lasso']['Betas'][0:14]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=getfinalresults(X_train,y_train,X_test,y_test)\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'Alpha', 'RMSE', 'R2'])\n",
    "plot_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    df_capstone_dropped_final['Proportion of students that said they would take the class again']\n",
    "]).T\n",
    "\n",
    "y = np.array(df_capstone_dropped_final['Average Rating'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "\n",
    "results=getKFresults(X_train,y_train)\n",
    "\n",
    "results_df2 = pd.DataFrame(results, columns=['Model', 'Betas','Alpha', 'RMSE', 'R2'])\n",
    "results_df=results_df2.drop(columns=['Betas'])\n",
    "plot_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mybetas=getbetas(X_train,y_train,X_test,y_test)[0] \n",
    "indices = np.argsort(mybetas)[::-1] \n",
    "print(indices)  \n",
    "print(getbetas(X_train,y_train,X_test,y_test))\n",
    "plot_betas(getbetas(X_train,y_train,X_test,y_test)[0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i,j) for i,j in zip(results_df2[results_df2['Model']=='Lasso']['Betas'][0:14],results_df2[results_df2['Model']=='Lasso']['RMSE'][0:14])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=getfinalresults(X_train,y_train,X_test,y_test)\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'Alpha', 'RMSE', 'R2'])\n",
    "plot_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagsdf=pd.read_csv('./rmpCapstoneTags.csv', header=None)\n",
    "Q8df=df_capstone[['Average Rating','Number of ratings']].join(tagsdf, how='inner')\n",
    "\n",
    "Q8df.dropna(inplace=True)\n",
    "Q8df\n",
    "for i in Q8df.columns[2:]:\n",
    "    Q8df[i] = Q8df[i].div(Q8df['Number of ratings'])\n",
    "Q8df\n",
    "Q8dfgreater10=Q8df[Q8df['Number of ratings'] >= 10]\n",
    "Q8dfgreater10\n",
    "corrgreater40=Q8dfgreater10.corr()[Q8dfgreater10.corr()>0.42]\n",
    "[i for i in list(corrgreater40[corrgreater40.notnull()].stack().index) if i[0]!=i[1]]\n",
    "correlation_matrix = Q8dfgreater10.drop(columns=['Number of ratings','Average Rating']).corr()\n",
    "plt.figure(figsize = (40,40))\n",
    "sns.heatmap(correlation_matrix,cmap = \"RdBu_r\", annot=True)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "dependent_var='Average Rating'\n",
    "for var in Q8dfgreater10.columns:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.scatterplot(x=Q8dfgreater10[var], y=Q8dfgreater10[dependent_var])\n",
    "    plt.title(f'Scatterplot of {dependent_var} vs {var}')\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel(dependent_var)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "Q8dfgreater10.corr()\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def plot_forward_selection_results(results_df):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # RMSE plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, len(results_df) + 1), results_df['RMSE'], marker='o', label='RMSE')\n",
    "    plt.title('Forward Feature Selection: RMSE')\n",
    "    plt.xlabel('Number of Features Selected')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # R² plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, len(results_df) + 1), results_df['R2'], marker='o', label='R²')\n",
    "    plt.title('Forward Feature Selection: R²')\n",
    "    plt.xlabel('Number of Features Selected')\n",
    "    plt.ylabel('R²')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def forward_feature_selection_kfold(X, y, k=5, max_features=None):\n",
    "    \"\"\"\n",
    "    Perform forward feature selection using k-fold cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - X: DataFrame of features\n",
    "    - y: Series of target variable\n",
    "    - k: Number of folds for cross-validation\n",
    "    - max_features: Max number of features to select. If None, select all.\n",
    "\n",
    "    Returns:\n",
    "    - selected_features: List of selected features\n",
    "    - results: List of results for each feature set (including average RMSE, R², etc.)\n",
    "    \"\"\"\n",
    "    remaining_features = list(X.columns)\n",
    "    selected_features = []\n",
    "    results = []\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    while remaining_features:\n",
    "        best_rmse = float('inf')\n",
    "        best_feature = None\n",
    "        best_betas = None\n",
    "        best_alpha = None\n",
    "        \n",
    "        for feature in remaining_features:\n",
    "            # Create a subset of the data with the current selected features + one more\n",
    "            X_temp = X[selected_features + [feature]]\n",
    "            \n",
    "            rmse_list = []\n",
    "            r2_list = []\n",
    "            \n",
    "            for train_idx, val_idx in kf.split(X_temp):\n",
    "                X_train_fold, X_val_fold = X_temp.iloc[train_idx], X_temp.iloc[val_idx]\n",
    "                y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "                \n",
    "                # Train a model\n",
    "                model = LinearRegression()\n",
    "                model.fit(X_train_fold, y_train_fold)\n",
    "                \n",
    "                # Make predictions and evaluate the model\n",
    "                y_pred_fold = model.predict(X_val_fold)\n",
    "                rmse_fold = np.sqrt(mean_squared_error(y_val_fold, y_pred_fold))\n",
    "                r2_fold = r2_score(y_val_fold, y_pred_fold)\n",
    "                \n",
    "                rmse_list.append(rmse_fold)\n",
    "                r2_list.append(r2_fold)\n",
    "            \n",
    "            # Calculate average RMSE and R² across folds\n",
    "            avg_rmse = np.mean(rmse_list)\n",
    "            avg_r2 = np.mean(r2_list)\n",
    "            \n",
    "            # Store the best feature and corresponding metrics\n",
    "            if avg_rmse < best_rmse:\n",
    "                best_rmse = avg_rmse\n",
    "                best_feature = feature\n",
    "                best_betas = model.coef_\n",
    "                best_alpha = model.intercept_\n",
    "        \n",
    "        # Update the list of selected features\n",
    "        selected_features.append(best_feature)\n",
    "        remaining_features.remove(best_feature)\n",
    "        \n",
    "        # Append the result for the current model\n",
    "        results.append([selected_features.copy(), best_betas, best_alpha, best_rmse, avg_r2])\n",
    "        \n",
    "        # Stop if we've reached the maximum number of features (if specified)\n",
    "        if max_features is not None and len(selected_features) >= max_features:\n",
    "            break\n",
    "\n",
    "    return selected_features, results\n",
    "X = Q8dfgreater10.drop(columns=['Average Rating','Number of ratings'])  # assuming all columns except 'Average Rating' are features\n",
    "y = Q8dfgreater10['Average Rating']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Perform forward feature selection with k-fold cross-validation\n",
    "selected_features_kfold, forward_results_kfold = forward_feature_selection_kfold(X_train, y_train, k=5)\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "forward_results_kfold_df = pd.DataFrame(forward_results_kfold, columns=['Selected Features', 'Betas', 'Alpha', 'RMSE', 'R2'])\n",
    "\n",
    "# Optionally, remove the 'Betas' column for cleaner display\n",
    "forward_results_kfold_df_cleaned = forward_results_kfold_df.drop(columns='Betas')\n",
    "\n",
    "# Plot the results\n",
    "plot_forward_selection_results(forward_results_kfold_df_cleaned)\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "best_selected_features_kfold = forward_results_kfold_df.iloc[-1]['Selected Features']\n",
    "X_train_best_kfold = X_train[best_selected_features_kfold]\n",
    "X_test_best_kfold = X_test[best_selected_features_kfold]\n",
    "\n",
    "final_model_kfold = LinearRegression()\n",
    "final_model_kfold.fit(X_train_best_kfold, y_train)\n",
    "y_pred_test_kfold = final_model_kfold.predict(X_test_best_kfold)\n",
    "\n",
    "final_rmse_kfold = np.sqrt(mean_squared_error(y_test, y_pred_test_kfold))\n",
    "final_r2_kfold = r2_score(y_test, y_pred_test_kfold)\n",
    "\n",
    "print(f\"Final Model with K-Fold RMSE: {final_rmse_kfold}\")\n",
    "print(f\"Final Model with K-Fold R²: {final_r2_kfold}\")\n",
    "\n",
    "forward_results_kfold_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    Q8dfgreater10[0],\n",
    "    Q8dfgreater10[1],\n",
    "    Q8dfgreater10[2],\n",
    "    Q8dfgreater10[3],\n",
    "    Q8dfgreater10[4],\n",
    "    Q8dfgreater10[5],\n",
    "    Q8dfgreater10[6],\n",
    "    Q8dfgreater10[7],\n",
    "    Q8dfgreater10[8],\n",
    "    Q8dfgreater10[9],\n",
    "    Q8dfgreater10[10],\n",
    "    Q8dfgreater10[11],\n",
    "    Q8dfgreater10[12],\n",
    "    Q8dfgreater10[13],\n",
    "    Q8dfgreater10[14],\n",
    "    Q8dfgreater10[15],\n",
    "    Q8dfgreater10[16],\n",
    "    Q8dfgreater10[17],\n",
    "    Q8dfgreater10[18],\n",
    "    Q8dfgreater10[19],\n",
    "]).T\n",
    "\n",
    "y = np.array(Q8dfgreater10['Average Rating'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "\n",
    "results=getKFresults(X_train,y_train)\n",
    "\n",
    "results_df2 = pd.DataFrame(results, columns=['Model','Betas','Alpha', 'RMSE', 'R2'])\n",
    "results_df = pd.DataFrame(results, columns=['Model','Betas','Alpha', 'RMSE', 'R2']).drop(columns='Betas')\n",
    "plot_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mybetas=getbetas(X_train,y_train,X_test,y_test)[0] \n",
    "indices = np.argsort(mybetas)[::-1] \n",
    "print(indices)  \n",
    "print(getbetas(X_train,y_train,X_test,y_test))\n",
    "plot_betas(getbetas(X_train,y_train,X_test,y_test)[0][1:],[i.name for i in [\n",
    "    Q8dfgreater10[0],\n",
    "    Q8dfgreater10[1],\n",
    "    Q8dfgreater10[2],\n",
    "    Q8dfgreater10[3],\n",
    "    Q8dfgreater10[4],\n",
    "    Q8dfgreater10[5],\n",
    "    Q8dfgreater10[6],\n",
    "    Q8dfgreater10[7],\n",
    "    Q8dfgreater10[8],\n",
    "    Q8dfgreater10[9],\n",
    "    Q8dfgreater10[10],\n",
    "    Q8dfgreater10[11],\n",
    "    Q8dfgreater10[12],\n",
    "    Q8dfgreater10[13],\n",
    "    Q8dfgreater10[14],\n",
    "    Q8dfgreater10[15],\n",
    "    Q8dfgreater10[16],\n",
    "    Q8dfgreater10[17],\n",
    "    Q8dfgreater10[18],\n",
    "    Q8dfgreater10[19],\n",
    "]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i,j) for i,j in zip(results_df2[results_df2['Model']=='Lasso']['Betas'][0:14],results_df2[results_df2['Model']=='Lasso']['RMSE'][0:14])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=getfinalresults(X_train,y_train,X_test,y_test)\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'Alpha', 'RMSE', 'R2'])\n",
    "plot_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    Q8dfgreater10[2]\n",
    "]).T\n",
    "X_sqrt = np.sqrt(X)   # Square root of the column\n",
    "X_square = np.square(X)  # Square of the column\n",
    "X_cube = np.power(X, 3)  # Cube of the column\n",
    "\n",
    "X_extended = np.hstack((X, X_sqrt, X_square, X_cube))\n",
    "X=X_extended\n",
    "y = np.array(Q8dfgreater10['Average Rating'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "results=getKFresults(X_train,y_train)\n",
    "\n",
    "results_df2 = pd.DataFrame(results, columns=['Model','Betas','Alpha', 'RMSE', 'R2'])\n",
    "results_df = pd.DataFrame(results, columns=['Model','Betas','Alpha', 'RMSE', 'R2']).drop(columns='Betas')\n",
    "plot_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    Q8dfgreater10[0]\n",
    "]).T\n",
    "\n",
    "y = np.array(Q8dfgreater10['Average Rating'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "results=getKFresults(X_train,y_train)\n",
    "\n",
    "results_df2 = pd.DataFrame(results, columns=['Model','Betas','Alpha', 'RMSE', 'R2'])\n",
    "results_df = pd.DataFrame(results, columns=['Model','Betas','Alpha', 'RMSE', 'R2']).drop(columns='Betas')\n",
    "plot_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[results_df['Model']=='Normal'].drop(columns=['Model','Alpha']).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getbetas(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    Q8dfgreater10[0],\n",
    "    Q8dfgreater10[1],\n",
    "    Q8dfgreater10[2],\n",
    "    Q8dfgreater10[15],\n",
    "    Q8dfgreater10[16],\n",
    "    Q8dfgreater10[12],\n",
    "]).T\n",
    "y = np.array(Q8dfgreater10['Average Rating'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "results=getKFresults(X_train,y_train)\n",
    "\n",
    "results_df2 = pd.DataFrame(results, columns=['Model','Betas','Alpha', 'RMSE', 'R2'])\n",
    "results_df = pd.DataFrame(results, columns=['Model','Betas','Alpha', 'RMSE', 'R2']).drop(columns='Betas')\n",
    "plot_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i,j) for i,j in zip(results_df2[results_df2['Model']=='Lasso']['Betas'][0:14],results_df2[results_df2['Model']=='Lasso']['RMSE'][0:14])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mybetas=getbetas(X_train,y_train,X_test,y_test)[0]\n",
    "indices = np.argsort(mybetas)[::-1]\n",
    "print(indices) \n",
    "print(getbetas(X_train,y_train,X_test,y_test))\n",
    "plot_betas(getbetas(X_train,y_train,X_test,y_test)[0][1:],[i.name for i in [\n",
    "    Q8dfgreater10[0],\n",
    "    Q8dfgreater10[1],\n",
    "    Q8dfgreater10[2],\n",
    "    Q8dfgreater10[15],\n",
    "    Q8dfgreater10[16],\n",
    "    Q8dfgreater10[12],\n",
    "]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=getfinalresults(X_train,y_train,X_test,y_test)\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'Alpha', 'RMSE', 'R2'])\n",
    "plot_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mybetas=getbetas(X_train,y_train,X_test,y_test)[0] \n",
    "indices = np.argsort(mybetas)[::-1] \n",
    "print(indices)  \n",
    "print(getbetas(X_train,y_train,X_test,y_test))\n",
    "plot_betas(getbetas(X_train,y_train,X_test,y_test)[0][1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Build a regression model predicting average difficulty from all tags (the ones in the rmpCapstoneTags.csv) file. Make sure to include the R2and RMSE of this model. Which of these tags is most strongly predictive of average difficulty? Hint: Make sure to address collinearity concerns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9df=df_capstone[['Average Difficulty','Number of ratings']].join(tagsdf, how='inner')\n",
    "Q9df.head(2)\n",
    "Q9df.dropna(inplace=True)\n",
    "Q9df.head(2)\n",
    "for i in Q9df.columns[2:]:\n",
    "    Q9df[i] = Q9df[i].div(Q9df['Number of ratings'])\n",
    "Q9df.head(2)\n",
    "Q9dfgreater10=Q9df[Q9df['Number of ratings'] >= 10]\n",
    "Q9dfgreater10.head(2)\n",
    "Q9dfgreater10.corr()\n",
    "corrgreater40=Q9dfgreater10.corr()[Q9dfgreater10.corr()>0.30]\n",
    "[i for i in list(corrgreater40[corrgreater40.notnull()].stack().index) if i[0]!=i[1]]\n",
    "dependent_var='Average Difficulty'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in Q9dfgreater10.columns:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.scatterplot(x=Q9dfgreater10[var], y=Q9dfgreater10[dependent_var])\n",
    "    plt.title(f'Scatterplot of {dependent_var} vs {var}')\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel(dependent_var)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "X = Q9dfgreater10.drop(columns=['Average Difficulty','Number of ratings'])  # assuming all columns except 'Average Rating' are features\n",
    "y = Q9dfgreater10['Average Difficulty']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "def plot_forward_selection_results(results_df):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # RMSE plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, len(results_df) + 1), results_df['RMSE'], marker='o', label='RMSE')\n",
    "    plt.title('Forward Feature Selection: RMSE')\n",
    "    plt.xlabel('Number of Features Selected')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # R² plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, len(results_df) + 1), results_df['R2'], marker='o', label='R²')\n",
    "    plt.title('Forward Feature Selection: R²')\n",
    "    plt.xlabel('Number of Features Selected')\n",
    "    plt.ylabel('R²')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def forward_feature_selection_kfold(X, y, k=5, max_features=None):\n",
    "    \"\"\"\n",
    "    Perform forward feature selection using k-fold cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - X: DataFrame of features\n",
    "    - y: Series of target variable\n",
    "    - k: Number of folds for cross-validation\n",
    "    - max_features: Max number of features to select. If None, select all.\n",
    "\n",
    "    Returns:\n",
    "    - selected_features: List of selected features\n",
    "    - results: List of results for each feature set (including average RMSE, R², etc.)\n",
    "    \"\"\"\n",
    "    remaining_features = list(X.columns)\n",
    "    selected_features = []\n",
    "    results = []\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    while remaining_features:\n",
    "        best_rmse = float('inf')\n",
    "        best_feature = None\n",
    "        best_betas = None\n",
    "        best_alpha = None\n",
    "        \n",
    "        for feature in remaining_features:\n",
    "            # Create a subset of the data with the current selected features + one more\n",
    "            X_temp = X[selected_features + [feature]]\n",
    "            \n",
    "            rmse_list = []\n",
    "            r2_list = []\n",
    "            \n",
    "            for train_idx, val_idx in kf.split(X_temp):\n",
    "                X_train_fold, X_val_fold = X_temp.iloc[train_idx], X_temp.iloc[val_idx]\n",
    "                y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "                \n",
    "                # Train a model\n",
    "                model = LinearRegression()\n",
    "                model.fit(X_train_fold, y_train_fold)\n",
    "                \n",
    "                # Make predictions and evaluate the model\n",
    "                y_pred_fold = model.predict(X_val_fold)\n",
    "                rmse_fold = np.sqrt(mean_squared_error(y_val_fold, y_pred_fold))\n",
    "                r2_fold = r2_score(y_val_fold, y_pred_fold)\n",
    "                \n",
    "                rmse_list.append(rmse_fold)\n",
    "                r2_list.append(r2_fold)\n",
    "            \n",
    "            # Calculate average RMSE and R² across folds\n",
    "            avg_rmse = np.mean(rmse_list)\n",
    "            avg_r2 = np.mean(r2_list)\n",
    "            \n",
    "            # Store the best feature and corresponding metrics\n",
    "            if avg_rmse < best_rmse:\n",
    "                best_rmse = avg_rmse\n",
    "                best_feature = feature\n",
    "                best_betas = model.coef_\n",
    "                best_alpha = model.intercept_\n",
    "        \n",
    "        # Update the list of selected features\n",
    "        selected_features.append(best_feature)\n",
    "        remaining_features.remove(best_feature)\n",
    "        \n",
    "        # Append the result for the current model\n",
    "        results.append([selected_features.copy(), best_betas, best_alpha, best_rmse, avg_r2])\n",
    "        \n",
    "        # Stop if we've reached the maximum number of features (if specified)\n",
    "        if max_features is not None and len(selected_features) >= max_features:\n",
    "            break\n",
    "\n",
    "    return selected_features, results\n",
    "\n",
    "# Perform forward feature selection with k-fold cross-validation\n",
    "selected_features_kfold, forward_results_kfold = forward_feature_selection_kfold(X_train, y_train, k=5)\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "forward_results_kfold_df = pd.DataFrame(forward_results_kfold, columns=['Selected Features', 'Betas', 'Alpha', 'RMSE', 'R2'])\n",
    "\n",
    "# Optionally, remove the 'Betas' column for cleaner display\n",
    "forward_results_kfold_df_cleaned = forward_results_kfold_df.drop(columns='Betas')\n",
    "\n",
    "# Plot the results\n",
    "plot_forward_selection_results(forward_results_kfold_df_cleaned)\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "best_selected_features_kfold = forward_results_kfold_df.iloc[-1]['Selected Features']\n",
    "X_train_best_kfold = X_train[best_selected_features_kfold]\n",
    "X_test_best_kfold = X_test[best_selected_features_kfold]\n",
    "\n",
    "final_model_kfold = LinearRegression()\n",
    "final_model_kfold.fit(X_train_best_kfold, y_train)\n",
    "y_pred_test_kfold = final_model_kfold.predict(X_test_best_kfold)\n",
    "\n",
    "final_rmse_kfold = np.sqrt(mean_squared_error(y_test, y_pred_test_kfold))\n",
    "final_r2_kfold = r2_score(y_test, y_pred_test_kfold)\n",
    "\n",
    "print(f\"Final Model with K-Fold RMSE: {final_rmse_kfold}\")\n",
    "print(f\"Final Model with K-Fold R²: {final_r2_kfold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_results_kfold_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    Q9dfgreater10[0],\n",
    "    Q9dfgreater10[1],\n",
    "    Q9dfgreater10[2],\n",
    "    Q9dfgreater10[3],\n",
    "    Q9dfgreater10[4],\n",
    "    Q9dfgreater10[5],\n",
    "    Q9dfgreater10[6],\n",
    "    Q9dfgreater10[7],\n",
    "    Q9dfgreater10[8],\n",
    "    Q9dfgreater10[9],\n",
    "    Q9dfgreater10[10],\n",
    "    Q9dfgreater10[11],\n",
    "    Q9dfgreater10[12],\n",
    "    Q9dfgreater10[13],\n",
    "    Q9dfgreater10[14],\n",
    "    Q9dfgreater10[15],\n",
    "    Q9dfgreater10[16],\n",
    "    Q9dfgreater10[17],\n",
    "    Q9dfgreater10[18],\n",
    "    Q9dfgreater10[19],\n",
    "]).T\n",
    "\n",
    "y = np.array(Q9dfgreater10['Average Difficulty'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "\n",
    "results=getKFresults(X_train,y_train)\n",
    "\n",
    "results_df2 = pd.DataFrame(results, columns=['Model','Betas','Alpha', 'RMSE', 'R2'])\n",
    "results_df = pd.DataFrame(results, columns=['Model','Betas','Alpha', 'RMSE', 'R2']).drop(columns='Betas')\n",
    "plot_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mybetas=getbetaslasso(X_train,y_train,X_test,y_test)[0]\n",
    "indices = np.argsort(mybetas)[::-1]\n",
    "print(indices) \n",
    "print(mybetas[indices]) \n",
    "print(getbetaslasso(X_train,y_train,X_test,y_test,alpha=1000))\n",
    "print(getbetas(X_train,y_train,X_test,y_test))\n",
    "plot_betas(getbetaslasso(X_train,y_train,X_test,y_test,alpha=1000)[0][1:],[i.name for i in [\n",
    "    Q9dfgreater10[0],\n",
    "    Q9dfgreater10[1],\n",
    "    Q9dfgreater10[2],\n",
    "    Q9dfgreater10[3],\n",
    "    Q9dfgreater10[4],\n",
    "    Q9dfgreater10[5],\n",
    "    Q9dfgreater10[6],\n",
    "    Q9dfgreater10[7],\n",
    "    Q9dfgreater10[8],\n",
    "    Q9dfgreater10[9],\n",
    "    Q9dfgreater10[10],\n",
    "    Q9dfgreater10[11],\n",
    "    Q9dfgreater10[12],\n",
    "    Q9dfgreater10[13],\n",
    "    Q9dfgreater10[14],\n",
    "    Q9dfgreater10[15],\n",
    "    Q9dfgreater10[16],\n",
    "    Q9dfgreater10[17],\n",
    "    Q9dfgreater10[18],\n",
    "    Q9dfgreater10[19],\n",
    "]],'Lasso')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i,j) for i,j in zip(results_df2[results_df2['Model']=='Lasso']['Betas'][0:14],results_df2[results_df2['Model']=='Lasso']['RMSE'][0:14])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=getfinalresults(X_train,y_train,X_test,y_test)\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'Alpha', 'RMSE', 'R2'])\n",
    "plot_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    Q9dfgreater10[0]\n",
    "]).T\n",
    "X_sqrt = np.sqrt(X)   # Square root of the column\n",
    "X_square = np.square(X)  # Square of the column\n",
    "X_cube = np.power(X, 3)  # Cube of the column\n",
    "\n",
    "X_extended = np.hstack((X, X_sqrt, X_square, X_cube))\n",
    "X=X_extended\n",
    "y = np.array(Q9dfgreater10['Average Difficulty'])\n",
    "#y = np.sqrt(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "results=getKFresults(X_train,y_train)\n",
    "\n",
    "results_df2 = pd.DataFrame(results, columns=['Model','Betas','Alpha', 'RMSE', 'R2'])\n",
    "results_df = pd.DataFrame(results, columns=['Model','Betas','Alpha', 'RMSE', 'R2']).drop(columns='Betas')\n",
    "plot_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=getfinalresults(X_train,y_train,X_test,y_test)\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'Alpha', 'RMSE', 'R2'])\n",
    "plot_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i,j) for i,j in zip(results_df2[results_df2['Model']=='Lasso']['Betas'][0:14],results_df2[results_df2['Model']=='Lasso']['RMSE'][0:14])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    Q9dfgreater10[0],\n",
    "    Q9dfgreater10[13],\n",
    "    Q9dfgreater10[6],\n",
    "    Q9dfgreater10[3],\n",
    "    Q9dfgreater10[9]\n",
    "]).T\n",
    "\n",
    "X_sqrt = np.sqrt(X)   # Square root of the column\n",
    "\n",
    "X_extended = np.hstack((X, X_sqrt))\n",
    "X=X_extended\n",
    "y = np.array(Q9dfgreater10['Average Difficulty'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "results=getKFresults(X_train,y_train)\n",
    "\n",
    "results_df2 = pd.DataFrame(results, columns=['Model','Betas','Alpha', 'RMSE', 'R2'])\n",
    "results_df = pd.DataFrame(results, columns=['Model','Betas','Alpha', 'RMSE', 'R2']).drop(columns='Betas')\n",
    "plot_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=getfinalresults(X_train,y_train,X_test,y_test)\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'Alpha', 'RMSE', 'R2'])\n",
    "plot_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mybetas=getbetaslasso(X_train,y_train,X_test,y_test,alpha=1000)[0]\n",
    "indices = np.argsort(mybetas)[::-1]\n",
    "print(indices) \n",
    "print(getbetaslasso(X_train,y_train,X_test,y_test,alpha=100))\n",
    "plot_betas(getbetaslasso(X_train,y_train,X_test,y_test,alpha=100)[0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    Q9dfgreater10[0],\n",
    "    Q9dfgreater10[13],\n",
    "    Q9dfgreater10[6],\n",
    "    Q9dfgreater10[3],\n",
    "    Q9dfgreater10[9]\n",
    "]).T\n",
    "\n",
    "y = np.array(Q9dfgreater10['Average Difficulty'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "results=getKFresults(X_train,y_train)\n",
    "\n",
    "results_df2 = pd.DataFrame(results, columns=['Model','Betas','Alpha', 'RMSE', 'R2'])\n",
    "results_df = pd.DataFrame(results, columns=['Model','Betas','Alpha', 'RMSE', 'R2']).drop(columns='Betas')\n",
    "plot_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=getfinalresults(X_train,y_train,X_test,y_test)\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'Alpha', 'RMSE', 'R2'])\n",
    "plot_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mybetas=getbetas(X_train,y_train,X_test,y_test)[0]\n",
    "indices = np.argsort(mybetas)[::-1]\n",
    "print(indices) \n",
    "print(getbetas(X_train,y_train,X_test,y_test))\n",
    "plot_betas(getbetas(X_train,y_train,X_test,y_test)[0][1:],[i.name for i in [\n",
    "    Q9dfgreater10[0],\n",
    "    Q9dfgreater10[13],\n",
    "    Q9dfgreater10[6],\n",
    "    Q9dfgreater10[3],\n",
    "    Q9dfgreater10[9]\n",
    "]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9dfgreater10.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    Q9dfgreater10[0],\n",
    "]).T\n",
    "\n",
    "y = np.array(Q9dfgreater10['Average Difficulty'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "results=getKFresults(X_train,y_train)\n",
    "\n",
    "results_df2 = pd.DataFrame(results, columns=['Model','Betas','Alpha', 'RMSE', 'R2'])\n",
    "results_df = pd.DataFrame(results, columns=['Model','Betas','Alpha', 'RMSE', 'R2']).drop(columns='Betas')\n",
    "plot_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mybetas=getbetas(X_train,y_train,X_test,y_test)[0]\n",
    "indices = np.argsort(mybetas)[::-1]\n",
    "print(indices) \n",
    "print(getbetas(X_train,y_train,X_test,y_test))\n",
    "plot_betas(getbetas(X_train,y_train,X_test,y_test)[0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=getfinalresults(X_train,y_train,X_test,y_test)\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'Alpha', 'RMSE', 'R2'])\n",
    "plot_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10df=df_capstone.join(tagsdf, how='inner')\n",
    "Q10df.columns\n",
    "Q10df.dropna(inplace=True)\n",
    "for i in Q10df.columns[8:]:\n",
    "    Q10df[i] = Q10df[i].div(Q10df['Number of ratings'])\n",
    "Q10df=Q10df[(Q10df['Male Professor']==1) & (Q10df['Female Professor']==0) | (Q10df['Male Professor']==0) & (Q10df['Female Professor']==1)]\n",
    "correlation_matrix = Q10df.corr()\n",
    "plt.figure(figsize = (40,40))\n",
    "sns.heatmap(correlation_matrix,cmap = \"RdBu_r\", annot=True)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    classification_report)\n",
    "# Let's try Average Rating\n",
    "averagerating = Q10df['Average Rating']\n",
    "receivedapepper = Q10df['Received a pepper']\n",
    "fig, ax = plt.subplots(figsize=(10,6)) # Could also do figure and plt.() later on, but subplots are a generalization\n",
    "\n",
    "ax.scatter(x=averagerating, y=receivedapepper, c='purple') # Purple for NYU :)\n",
    "ax.set_title(\"Scatterplot of Average Rating V Received a pepper\")\n",
    "ax.set_xlabel(\"Average Rating (X)\")\n",
    "ax.set_ylabel(\"Received a pepper (Y)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# We definitely shouldn't draw a line through this lol\n",
    "# Logistic Regression with one independent variable\n",
    "X = Q10df[['Average Rating']] # Double bracket for shaping (not a worry for multiple logistic regression)\n",
    "y = Q10df['Received a pepper']\n",
    "\n",
    "# Train-test split from scikit learn\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=seed\n",
    ")\n",
    "\n",
    "print(X_train) # see training data\n",
    "\n",
    "print(y_train) # see target\n",
    "\n",
    "# Fit logistic regression\n",
    "log_reg_single = LogisticRegression()\n",
    "log_reg_single.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "# Class\n",
    "y_pred = log_reg_single.predict(X_test)\n",
    "\n",
    "# Probabilities\n",
    "y_prob = log_reg_single.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Threshold?\n",
    "# Get predictions in dataframe\n",
    "results = pd.DataFrame({'Predictions': y_pred, 'Probabilities': y_prob})\n",
    "print(results.tail())  # Display a few rows\n",
    "\n",
    "print(results[results['Predictions'] == 1].min())\n",
    "print(results[results['Predictions'] == 0].max())\n",
    "\n",
    "THRESHOLD = 0.607 # Revisit later!\n",
    "# THRESHOLD = optimal_threshold # from ROC curve\n",
    "y_pred_new = (y_prob > THRESHOLD).astype(int)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "# Precision = TP / TP + FP\n",
    "# Recall = TP / TP + FN\n",
    "# F1 = 2 * P * R / (P + R) - harmonic mean, balance precision and recall\n",
    "# Support - data pts in the class\n",
    "# Macro metrics - average over each class\n",
    "# Micro or weighted metrics - treat each sample the same\n",
    "\n",
    "class_report = classification_report(y_test, y_pred_new) #y_pred_new\n",
    "print(class_report)\n",
    "\n",
    "# Interpret coefficients\n",
    "print(log_reg_single.coef_)\n",
    "# Interpret: For every increase in 1 unit of Average Rating, we expect the odds of Received a pepper relative to odds of no Received a pepper (the ratio) to increase by e^0.03\n",
    "print(np.exp(log_reg_single.coef_[0])) # Slight boost for odds of Received a pepper\n",
    "\n",
    "print(log_reg_single.intercept_)\n",
    "print(np.exp(log_reg_single.intercept_)) # Not super interpretable, Average Rating wouldn't be 0. But \"base\" odds here.\n",
    "# If Average Rating was 0, p / 1 -p or Received a pepper v no Received a pepper would be small odds\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_single = confusion_matrix(y_test, y_pred) #y_pred_new\n",
    "sns.heatmap(conf_matrix_single, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=[\"0 (Did not)\", \"1 (Received a pepper)\"], \n",
    "            yticklabels=[\"0 (Did not)\", \"1 (Received a pepper)\"])\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "# What do you think doc?\n",
    "\n",
    "# Visualize the curve\n",
    "# Extract coefficients\n",
    "beta1 = log_reg_single.coef_[0][0]\n",
    "intercept = log_reg_single.intercept_[0]\n",
    "\n",
    "# Apply over training data\n",
    "x_vals = np.linspace(X_train.min(), X_train.max(), 100)\n",
    "\n",
    "# Logistic reg formula\n",
    "y_vals = 1 / (1 + np.exp(-(beta1 * x_vals + intercept)))\n",
    "plt.plot(x_vals, y_vals, label=\"Sigmoid Curve\")\n",
    "\n",
    "# Add threshold line\n",
    "threshold = THRESHOLD\n",
    "threshold_x = (np.log(threshold / (1 - threshold)) - intercept) / beta1  # Solve for x when sigmoid(x) = threshold\n",
    "plt.axvline(threshold_x, color='red', linestyle='--', label=f'Threshold at Average Rating = {threshold_x:.2f}')\n",
    "plt.title(\"Visualizing the Curve\")\n",
    "plt.xlabel(\"Average Rating\")\n",
    "plt.ylabel(\"Probability of Received a pepper\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve, get AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "plt.ylabel(\"True Positive Rate (Sensitivity)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Try to find optimal threshold for fitting\n",
    "optimal_threshold_index = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_threshold_index]\n",
    "print(optimal_threshold)\n",
    "\n",
    "# List of chosen thresholds\n",
    "chosen_thresholds = [0.2, 0.3, 0.5]\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n",
    "\n",
    "# Loop over each threshold\n",
    "for chosen_threshold in chosen_thresholds:\n",
    "    # Find index of the threshold closest to the chosen one\n",
    "    threshold_index = (np.abs(thresholds - chosen_threshold)).argmin()\n",
    "\n",
    "    # Get the corresponding FPR and TPR for the chosen threshold\n",
    "    fpr_at_threshold = fpr[threshold_index]\n",
    "    tpr_at_threshold = tpr[threshold_index]\n",
    "\n",
    "    # Plot the chosen threshold point on the ROC curve\n",
    "    plt.scatter(fpr_at_threshold, tpr_at_threshold, label=f\"Threshold = {chosen_threshold}\", zorder=5)\n",
    "\n",
    "# Add title, labels, and legend\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "plt.ylabel(\"True Positive Rate (Sensitivity)\")\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    classification_report\n",
    ")\n",
    "X = Q10df.drop(columns=['Received a pepper','Number of ratings','Number of ratings coming from online classes','Male Professor','Female Professor',4,8,9,17,18])  # assuming all columns except 'Average Rating' are features\n",
    "X.columns =X.columns.astype(str)\n",
    "# Multiple\n",
    "# Apply min-max scaling to get variables on same scale\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X) # Row-wise\n",
    "print(X)\n",
    "print(X.shape)\n",
    "\n",
    "# Logistic Regression with One Explanatory Variable\n",
    "y = Q10df['Received a pepper']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=seed\n",
    ")\n",
    "\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "# Fit logistic regression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = log_reg.predict(X_test)\n",
    "y_prob = log_reg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Efficiently create and display the DataFrame\n",
    "results = pd.DataFrame({'Predictions': y_pred, 'Probabilities': y_prob})\n",
    "print(results.tail())  # Display a few rows\n",
    "\n",
    "print(results[results['Predictions'] == 1].min())\n",
    "print(results[results['Predictions'] == 0].max())\n",
    "\n",
    "THRESHOLD = 0.465 # Revisit later!\n",
    "# THRESHOLD = optimal_threshold\n",
    "y_pred_new = (y_prob > THRESHOLD).astype(int)\n",
    "\n",
    "class_report = classification_report(y_test, y_pred_new) #y_pred_new\n",
    "print(class_report)\n",
    "\n",
    "print(log_reg.coef_)\n",
    "print(np.exp(log_reg.coef_)) # These are huge. But careful, we scaled our X variables (between 0 and 1). Still interpretable over fractional units.\n",
    "\n",
    "print(log_reg.intercept_)\n",
    "print(np.exp(log_reg.intercept_))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred) #y_pred_new\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=[\"0 (No Pepper)\", \"1 (Pepper)\"], \n",
    "            yticklabels=[\"0 (No Pepper)\", \"1 (Pepper)\"])\n",
    "\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "plt.ylabel(\"True Positive Rate (Sensitivity)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "optimal_threshold_index = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_threshold_index]\n",
    "print(optimal_threshold)\n",
    "\n",
    "# List of chosen thresholds\n",
    "chosen_thresholds = [0.488]\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n",
    "\n",
    "# Loop over each threshold\n",
    "for chosen_threshold in chosen_thresholds:\n",
    "    # Find index of the threshold closest to the chosen one\n",
    "    threshold_index = (np.abs(thresholds - chosen_threshold)).argmin()\n",
    "\n",
    "    # Get the corresponding FPR and TPR for the chosen threshold\n",
    "    fpr_at_threshold = fpr[threshold_index]\n",
    "    tpr_at_threshold = tpr[threshold_index]\n",
    "\n",
    "    # Plot the chosen threshold point on the ROC curve\n",
    "    plt.scatter(fpr_at_threshold, tpr_at_threshold, label=f\"Threshold = {chosen_threshold}\", zorder=5)\n",
    "\n",
    "# Add title, labels, and legend\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "plt.ylabel(\"True Positive Rate (Sensitivity)\")\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "svm_model = SVC(kernel='linear', probability=True, random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions and classification report\n",
    "y_pred = svm_model.predict(X_test)\n",
    "y_prob = svm_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Coefficients and intercept for hyperplane\n",
    "w = svm_model.coef_[0]\n",
    "b = svm_model.intercept_[0]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "plt.ylabel(\"True Positive Rate (Sensitivity)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
